<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compound Engineering: When Two Tools Are Better Than One</title>
    <style>
        :root {
            --bg: #fafafa;
            --text: #1a1a1a;
            --accent: #2563eb;
            --muted: #6b7280;
            --code-bg: #1e293b;
            --code-text: #e2e8f0;
            --table-border: #e5e7eb;
            --blockquote-bg: #f3f4f6;
        }

        * { box-sizing: border-box; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            max-width: 680px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        h1 {
            font-size: 2rem;
            line-height: 1.2;
            margin-bottom: 0.5rem;
        }

        h2 {
            font-size: 1.4rem;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            border-bottom: 2px solid var(--accent);
            padding-bottom: 0.5rem;
        }

        .subtitle {
            color: var(--muted);
            font-style: italic;
            font-size: 1.1rem;
            margin-bottom: 2rem;
        }

        hr {
            border: none;
            border-top: 1px solid var(--table-border);
            margin: 2rem 0;
        }

        p { margin: 1rem 0; }

        strong { font-weight: 600; }

        code {
            font-family: 'SF Mono', Monaco, 'Courier New', monospace;
            background: #e5e7eb;
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-size: 0.9em;
        }

        pre {
            background: var(--code-bg);
            color: var(--code-text);
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            line-height: 1.5;
        }

        pre code {
            background: none;
            padding: 0;
            color: inherit;
        }

        blockquote {
            background: var(--blockquote-bg);
            border-left: 4px solid var(--accent);
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            font-style: italic;
        }

        blockquote p { margin: 0.5rem 0; }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        th, td {
            border: 1px solid var(--table-border);
            padding: 0.75rem;
            text-align: left;
        }

        th {
            background: #f9fafb;
            font-weight: 600;
        }

        ol, ul {
            margin: 1rem 0;
            padding-left: 1.5rem;
        }

        li { margin: 0.5rem 0; }

        a {
            color: var(--accent);
            text-decoration: none;
        }

        a:hover { text-decoration: underline; }

        .footer {
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--table-border);
            color: var(--muted);
            font-size: 0.9rem;
            font-style: italic;
        }

        .insight-box {
            background: linear-gradient(135deg, #eff6ff 0%, #f0fdf4 100%);
            border: 1px solid #93c5fd;
            border-radius: 8px;
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
        }

        .insight-box strong {
            color: var(--accent);
        }
    </style>
</head>
<body>
    <article>
        <h1>Compound Engineering: When Two Tools Are Better Than One</h1>
        <p class="subtitle">How Claude helped me evaluate, compare, and combine two CLI tools without reading a single line of source code</p>

        <hr>

        <h2>The Setup</h2>
        <p>It's late evening, and I'm deep in a familiar pattern: testing new tools that promise to improve my Claude Code workflow. Tonight's candidates are two independent projects from different developers:</p>
        <ol>
            <li><strong>pchalasani/claude-code-tools</strong> — A session management toolkit with search, resume, and context preservation</li>
            <li><strong>simonw/claude-code-transcripts</strong> — A session export tool that converts JSONL transcripts to shareable HTML</li>
        </ol>
        <p>I discovered the second tool the way I discover most useful things: <a href="https://simonwillison.net/2025/Dec/25/claude-code-transcripts/">Simon Willison's blog</a>. Simon is one of my favorite tinkerers — I read basically everything he posts. His Christmas Day write-up about <code>claude-code-transcripts</code> landed in my feed while I was already mid-experiment with <code>aichat</code>. Perfect timing for a comparison.</p>
        <p>Both tools operate on the same data (Claude Code session files), which immediately raised a question: <em>Are these redundant? Competing? Or complementary?</em></p>
        <p>This is the kind of evaluation that used to take hours of documentation reading, trial-and-error, and sometimes diving into source code. Tonight, it took about 20 minutes — and I never opened a single source file.</p>

        <h2>The Compound Engineering Problem</h2>
        <p>Modern development increasingly involves <strong>compound engineering</strong> — combining existing tools, libraries, and services rather than building from scratch. The value isn't in any single tool; it's in how they compose.</p>
        <p>But composition creates complexity:</p>
        <ul>
            <li>Which tools overlap?</li>
            <li>Where do they conflict?</li>
            <li>How do their mental models align (or clash)?</li>
            <li>What's the integration surface?</li>
        </ul>
        <p>Traditional approaches to answering these questions:</p>
        <ol>
            <li><strong>Read all documentation</strong> — Time-consuming, often incomplete</li>
            <li><strong>Read source code</strong> — Even more time-consuming, requires understanding two codebases</li>
            <li><strong>Trial and error</strong> — Install both, break things, figure out why</li>
        </ol>
        <p>There's a fourth approach that AI collaboration enables: <strong>semantic evaluation</strong> — understanding tools by their <em>purpose</em> and <em>behavior</em> rather than their implementation.</p>

        <h2>Testing the First Tool: aichat</h2>
        <p>I started with claude-code-tools, specifically its <code>aichat</code> command. Installation was straightforward:</p>
        <pre><code>uv tool install claude-code-tools
brew install pchalasani/tap/aichat-search</code></pre>
        <p>The first impressive moment came when I built the search index:</p>
        <pre><code>Building search index...
Total: 20,639 session files to check
Indexed 3,119 new/modified sessions</code></pre>
        <p>Twenty thousand sessions. I've been using Claude Code for months, and this tool just made all that accumulated knowledge searchable. A quick test:</p>
        <pre><code>aichat search --json "helloworld worktree"</code></pre>
        <p>Returned rich JSON with session metadata, snippets, timestamps. The search uses Tantivy (a Rust-based search engine), so it's genuinely fast full-text search — not just filename matching.</p>

        <h2>The Overlap Question</h2>
        <p>Here's where it got interesting. I mentioned to Claude that I was also testing <code>claude-code-transcripts</code> in another window. The natural question: <em>Do these tools step on each other?</em></p>
        <p>Rather than reading documentation or source code for both tools, I asked Claude to analyze them based on what we'd observed:</p>
        <pre><code>can you look at the other experiment im working on in another window,
and check if the two tools can work together or if they are redundant</code></pre>
        <p>Claude examined both tools — not their code, but their <em>purpose</em>:</p>
        <table>
            <tr><th>Aspect</th><th><code>aichat</code></th><th><code>claude-code-transcripts</code></th></tr>
            <tr><td>Purpose</td><td>Session <strong>management</strong></td><td>Session <strong>export/sharing</strong></td></tr>
            <tr><td>Primary use</td><td>Internal workflow</td><td>External communication</td></tr>
            <tr><td>Output</td><td>Search index</td><td>Beautiful HTML</td></tr>
            <tr><td>Who benefits</td><td>Me (finding past work)</td><td>Others (sharing work)</td></tr>
        </table>
        <p>The key insight emerged: <strong>one finds, one shares</strong>.</p>

        <h2>The Grep vs Cat Analogy</h2>
        <p>Claude offered an analogy that crystallized the relationship:</p>
        <blockquote>
            <p>They're like <code>grep</code> vs <code>cat</code> — one finds things, one displays them.</p>
        </blockquote>
        <p>This is the kind of insight that's hard to extract from documentation. Documentation tells you <em>what</em> a tool does. It rarely tells you <em>where it fits</em> in a broader workflow alongside other tools.</p>
        <p>The compound engineering insight:</p>
        <pre><code>aichat search "canvas rendering bug"  →  finds session abc123
claude-code-transcripts abc123 --gist  →  shares that session</code></pre>
        <p>They're not just non-redundant — they're <strong>complementary stages</strong> in a workflow:</p>
        <ol>
            <li><strong>Find</strong> (aichat): "What sessions exist about X?"</li>
            <li><strong>Share</strong> (transcripts): "Let me export this one for my article"</li>
        </ol>

        <h2>Personalized Recommendations Without Code Review</h2>
        <p>Here's where the AI collaboration became particularly valuable. I asked whether I should even use these tools given my specific patterns:</p>
        <pre><code>do you recommend this tool for me? given my usage patterns?</code></pre>
        <p>Claude examined my actual session statistics:</p>
        <pre><code>Top 5 cwds:
  1495 | /Users/vishal/code/tldraw
   768 | /Users/vishal/code/helloworld
   284 | .../venturebot-v1/athens</code></pre>
        <p>1,495 sessions in tldraw alone. That's institutional knowledge I'd accumulated without realizing it. The recommendation became contextual:</p>
        <blockquote>
            <p>Your 1,495 tldraw sessions are a goldmine of context. Next time you're debugging something in tldraw, try: <code>aichat search --json "canvas rendering bug"</code>. You might find you solved a similar problem 6 months ago.</p>
        </blockquote>
        <p>This is compound engineering at the meta-level — not just "do these tools work together?" but "do these tools work together <em>for me</em>?"</p>

        <h2>Managing Complexity Without Implementation Details</h2>
        <p>What struck me about this evaluation process was what we <em>didn't</em> do:</p>
        <ul>
            <li><strong>Didn't read source code</strong> for either tool</li>
            <li><strong>Didn't trace data flows</strong> through implementations</li>
            <li><strong>Didn't debug integration issues</strong> at the code level</li>
            <li><strong>Didn't compare API signatures</strong> or internal architectures</li>
        </ul>
        <p>Instead, we evaluated at the level of:</p>
        <ul>
            <li><strong>Purpose</strong>: What problem does each solve?</li>
            <li><strong>Workflow fit</strong>: Where does each belong in my process?</li>
            <li><strong>Complementarity</strong>: Do they compete or compose?</li>
            <li><strong>Personal relevance</strong>: Given <em>my</em> usage, which pieces matter?</li>
        </ul>
        <p>This is semantic evaluation — understanding tools by meaning rather than mechanism.</p>

        <h2>The Meta-Pattern</h2>
        <p>This session revealed a pattern for compound engineering evaluation:</p>
        <ol>
            <li><strong>Test tools independently</strong> — Understand what each does in isolation</li>
            <li><strong>Ask the overlap question</strong> — Do they compete or compose?</li>
            <li><strong>Seek analogies</strong> — What established patterns do they follow?</li>
            <li><strong>Personalize the evaluation</strong> — Given <em>your</em> context, what matters?</li>
            <li><strong>Document the synthesis</strong> — Capture the combination, not just the components</li>
        </ol>

        <h2>Why This Matters</h2>
        <p>We're entering an era where the value increasingly comes from <strong>composition</strong> rather than creation. The tools exist. The challenge is:</p>
        <ul>
            <li>Discovering what's available</li>
            <li>Understanding how pieces fit</li>
            <li>Avoiding redundancy</li>
            <li>Maximizing synergy</li>
        </ul>
        <p>AI collaboration changes the economics of this evaluation. What used to require hours of documentation review or code reading can now happen through conversation — testing, comparing, synthesizing — in minutes.</p>
        <p>The specific tools don't matter (though <code>aichat</code> and <code>claude-code-transcripts</code> are both excellent). What matters is the pattern: <strong>compound engineering through semantic evaluation</strong>.</p>

        <hr>

        <h2>Quick Reference</h2>
        <p><strong>Install (persistent):</strong></p>
        <pre><code>uv tool install claude-code-tools
brew install pchalasani/tap/aichat-search</code></pre>
        <p><strong>Use (ephemeral):</strong></p>
        <pre><code>uvx claude-code-transcripts local --gist</code></pre>
        <p><strong>The workflow:</strong></p>
        <pre><code>aichat search "topic"  →  find relevant session
transcripts --gist     →  share it with others</code></pre>

        <div class="insight-box">
            <strong>The insight:</strong><br>
            <code>aichat</code> = "What sessions exist about X?" (for you)<br>
            <code>transcripts</code> = "How can I share this session?" (for others)
        </div>

        <hr>

        <h2>See It In Action</h2>
        <p>Want to see the actual session that produced this article? Here's the full transcript, exported using the very tool we discussed:</p>
        <p><strong><a href="https://gistpreview.github.io/?8cd18b1253910d75de05475da5afeea8/index.html">View the session transcript →</a></strong></p>
        <p>This is compound engineering eating its own tail — the session where we tested <code>claude-code-transcripts</code> was itself exported using <code>claude-code-transcripts</code>.</p>

        <hr>

        <p class="footer">This article was written collaboratively with Claude, documenting a real evaluation session. The tools mentioned are <a href="https://github.com/pchalasani/claude-code-tools">pchalasani/claude-code-tools</a> and <a href="https://github.com/simonw/claude-code-transcripts">simonw/claude-code-transcripts</a>.</p>
    </article>
</body>
</html>
