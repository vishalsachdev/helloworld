Chapter 1
At Home in the Universe
Out my window, just west of Santa Fe, lies the near spiritual landscape of northern New Mexico—barrancas, mesas, holy lands, the Rio Grande—home to the oldest civilization in North America. So much, so ancient and modern, pregnant with the remote past and the next millennium mingle here, haphazardly, slightly drunk with anticipation. Forty miles away lies Los Alamos, brilliance of mind, brilliance of flashing light that desert dawning in 1945, half a century ago, half our assumptions ago. Just beyond spreads the Valle Grande, remains of an archaic mountain said to have been over 30,000 feet high that blew its top, scattering ash to Arkansas, leaving obsidian for later, finer workings.
Some months ago, I found myself at lunch with Gunter Mahler, a theoretical physicist from Munich visiting the Santa Fe Institute, where a group of colleagues and I are engaged in a search for laws of complexity that would explain the strange patterns that spring up around us. Gunter looked northward, past pinon and juniper, taking in the long view toward Colorado, and somewhat astonished me by asking what my image of paradise was. As I groped for an answer, he proposed one: not the high mountains, or the ocean's edges, or flat lands. Rather, he suggested, just such terrain as lay before us, long and rolling under strong light, far ranges defining a distant horizon toward which graceful and telling land forms march in
    3
 fading procession. For reasons I do not completely understand, I felt he was right. We soon fell to speculations about the landscape of East Africa, and wondered whether, in fact, we might conceivably carry some genetic memory of our birthplace, our real Eden, our first home.
What stories we tell ourselves, of origins and endings, of form and transformation, of gods, the word, and law. All people, at all times,
must have created myths and stories to sketch a picture of our place under the sun. Cro-Magnon man, whose paintings of animals seem to exhibit a respect and awe, let alone line and form, that equals or surpasses those of later millennia, must have spun answers to these questions: Who are we? Where did we come from? Why are we here? Did Neanderthal, Homo habilis, or Homo erectus ask? Around which fire in the past 3 million years of hominid evolution did these questions first arise? Who knows.
Somewhere along our path, paradise has been lost, lost to the Western mind, and in the spreading world civilization, lost to our collective mind. John Milton must have been the last superb poet of Western civilization who could have sought to justify the ways of God to man in those early years foreshadowing the modern era. Paradise has been lost, not to sin, but to science. Once, a scant few centuries ago, we of the West believed ourselves the chosen of God, made in his image, keeping his word in a creation wrought by his love for us. Now, only 400 years later, we find ourselves on a tiny planet, on the edge of a humdrum galaxy among billions like it scattered across vast megaparsecs, around the curvature of space-time back to the Big Bang. We are but accidents, we're told. Purpose and value are ours alone to make. Without Satan and God, the universe now appears the neutral home of matter, dark and light, and is utterly indifferent. We bustle, but are no longer at home in the ancient sense.
We accept, of course, that the rise of science and the consequent technological explosion has driven us to our secular worldview Yet a spiritual hunger remains. I recently met N. Scott Momaday, a Native American author, winner of the Pulitzer Prize, at a small meeting in northern New Mexico intended to try to articulate the fundamental issues facing humanity. (As if a small group of thinkers could possibly succeed.) Momaday told us that the central issue we confront is to reinvent the sacred. He told of a sacred shield of the Kiowa, sanctified by the sacrifices and suffering of the warriors who had been honored to hold it in battle. The shield had been stolen following a battle with U.S. cavalry forces after the Civil War. He told us of the recent discovery and return of the shield from the family home of the post-Civil War general who had taken it. Momaday deep voice fell gently over us as he described the welcome set forth for that shield and the place it holds, quiet, somber, still, revered for the passion and suffering distilled within its arc.
Momaday's search for the sacred settled deep on me, for I hold the hope that what some are calling the new sciences of complexity may help us find anew our place in the universe, that through this new science,
we may recover our sense of worth, our sense of the sacred, just as the Kiowa ultimately recovered that sacred shield. At the same meeting, I suggested that the most important problem confronting humanity was the emergence of a world civilization, its profound promise, and the cultural dislocations this transformation will cause. To undergird the pluralistic global community that is aborning, we shall need, I think, an expanded intellectual basis—a new way to think about origins, evolution, and the profound naturalness of life and its myriad patterns of unfolding. This book is an effort to contribute to that new view, for the emerging sciences of complexity, as we shall see, offer fresh support for the idea of a pluralistic democratic society, providing evidence that it is not merely a human creation but part of the natural order of things. One is always wary of deducing from first principles the political order of one's own society. The nineteenth- century philosopher James Mill once succeeded in deducing from first principles that a constitutional monarchy, remarkably like that in England early in the last century, was the highest natural form of governance. But, as I hope to show, the very laws of complexity my colleagues and I are seeking suggest that democracy has evolved as perhaps the optimal mechanism to achieve the best attainable compromises among conflicting practical, political, and moral interests. Momaday must be right as well. We shall also need to reinvent the sacred—this sense of our own deep worth—and reinvest it at the core of the new civilization.
The story of our loss of paradise is familiar but worth retelling. Until Copernicus, we believed ourselves to be at the center of the universe. Nowadays, in our proclaimed sophistication, we look askance at a church that sought to suppress a heliocentric view. Knowledge for knowledge's sake, we say. Yes, of course. But was the church's concern with the disruption of a moral order really no more than a narrow vanity? To pre-Copernican Christian civilization, the geocentric view was no mere matter of science. Rather, it was the cornerstone evidence that the entire universe revolved around us. With God, angels, man, the beasts, and fertile plants made for our benefit, with the sun and stars wheeling overhead, we
    4

 knew our place: at the center of God's creation. The church feared rightly that the Copernican views would ultimately dismantle the unity of a thousand-year-old tradition of duty and rights, of obligations and roles, of moral fabric.
Copernicus blew his society open. Galileo and Kepler did not help much either, particularly Kepler, with his demonstration that planets orbit in ellipses rather than in the rational perfect circles envisioned by Aristotle. Kepler is such a wonderful transitional figure, a descendant of
the tradition of the magus, or great magi a century earlier. He had not sought ellipses, but rather harmonic orbits corresponding to the five perfect solids, from which Plato himself tried to build the world.
Then Newton, hero to us all, escaped the plague and wrenched Everyman into a universe even farther from paradise. What a giant step he took. Just imagine what it must have felt like to Newton as his new laws of mechanics took form in his mind. What wonder he must have felt. With a mere three laws oî motion and a universal law of gravitation in hand, Newton not only derived tides and orbits, but unleashed on the Western mind a clockwork universe. Before Newton, a scholastic philosopher, certain that an arrow arced toward its target because, as Aristotle taught, it was constantly acted on by a mysterious force, or impetus, could easily believe in a God who also moved things along by according them his sustained attention. Such a God might look after one if properly addressed. Such a God might return one to paradise. But after Newton, the laws alone sufficed. The universe could be wound up by God and released, thereafter left to tick inevitably toward eternity under the unfolding of his law, without further intervention. If the stars and tides moved without divine intervention, thinking people began to find it more difficult to hope for such intervention in their own affairs.
But there was some consolation. If the planets and all of inanimate matter obey eternal laws, then surely living things, with man at their summit, sitting at the top of the great chain of being, must reflect God's intention. Adam himself had named them all: insects, fish, reptiles, birds, mammals, man. Like the hierarchy of the church itself—laity, priest, bishop, archbishop, pope, saints, angels—the great chain of being stretched from the lowliest to the Almighty.
How Darwin and his theory of evolution by natural selection devastated all this ! We, who are heritors of Darwin, who see the living world through the categories he taught us more than a century ago, even we have trouble with the implications: man as the result of a chain of accidental mutations, sifted by a law no more noble than survival of the fittest. Creation science is no accident in late-twentieth-century America. Its proponents adhere to it in an ardent effort to forestall the feared moral implications of humans as descendants of a haphazard lineage branching from some last common ancestor more remote in time than the Cambrian explosion some 500 million years ago. The science in creation science is no science at all, but is the moral anguish so foolish? Or should creationism be viewed rather more sympathetically—misguided, to be sure, but part of a broader quest to reinvent the sacred in our secular world?
Before Darwin, the so-called Rational Morphologists found comfort
in the view that species were not the result of random mutation and selection, but of timeless laws of form. The finest eighteenth- and early-nineteenth-century biologists, comparing living forms, classified them into the hierarchical groupings of Linnean taxonomy that remain with us today: species, genera, families, orders, classes, phyla, and kingdoms—an order as natural and cosmically ordained to the scientists of that time as the great chain of being was to the Catholic church. Given the obvious morphological overlaps, these scientists sought lawful explanations of the similarities and differences. An analogy that helps us understand this aim can be found by thinking of crystals, which can exist only in certain forms. The Rational Morphologists, confronting fixed but highly similar species, sought similar regularities. Surely the pectoral fin of a fish, the bones of a petrel's wings, and the flashing legs of a horse were expressions of the same deep principle.
Darwin devastated this world. Species are not fixed by the squares of the Linnean chart; they evolve from one another. Natural selection acting on random variation, not God or some principle of Rational Morphology, accounted for the similarity of limb and fin, for creatures so marvelously attuned to their environments. The implications of these ideas, as understood by biologists today, have transformed humans, along with all other living forms, from works of a creator into ultimately historical accidents wrought by the opportunism of evolution. "Evolution is chance caught on the wing," wrote the biologist Jacques Monod. Evolution tinkers together contraptions, agreed François Jacob, the French geneticist who shared the Nobel with Monod. In the Judeo-Christian tradition, we had become used to thinking of ourselves as fallen angels. At least fallen angels have some hope of redemption and grace, of climbing back up the ecclesiastical ladder. Evolution left us stuck on the earth with no ladder to climb, contemplating our fate as nature's Rube Goldberg machines.
    5

 Random variation, selection sifting. Here is the core, the root. Here lies the brooding sense of accident, of historical contingency, of design by elimination. At least physics, cold in its calculus, implied a deep order, an inevitability. Biology has come to seem a science of the accidental, the ad hoc, and we just one of the fruits of this ad hocery Were the tape played over, we like to say, the forms of organisms would surely differ dramatically. We humans, a trumped-up, tricked- out, horn-blowing, self-important presence on the globe, need never have occurred. So much for our pretensions; we are lucky to have our hour. So much, too, for paradise.
Where, then, does this order come from, this teeming life I see from my window: urgent spider making her living with her pre-nylon web,
coyote crafty across the ridgetop, muddy Rio Grande aswarm with no-see-ems (an invisible insect peculiar to early evenings)? Since Darwin, we turn to a single, singular force, Natural Selection, which we might as well capitalize as though it were the new diety. Random variation, selection-sifting. Without it, we reason, there would be nothing but incoherent disorder.
I shall argue in this book that this idea is wrong. For, as we shall see, the emerging sciences of complexity begin to suggest that the order is not all accidental, that vast veins of spontaneous order lie at hand. Laws of complexity spontaneously generate much of the order of the natural world. It is only then that selection comes into play, further molding and refining. Such veins of spontaneous order have not been entirely unknown, yet they are just beginning to emerge as powerful new clues to the origins and evolution of life. We have all known that simple physical systems exhibit spontaneous order: an oil droplet in water forms a sphere; snowflakes exhibit their evanescent sixfold symmetry. What is new is that the range of spontaneous order is enormously greater than we have supposed. Profound order is being discovered in large, complex, and apparently random systems. I believe that this emergent order underlies not only the origin of life itself, but much of the order seen in organisms today. So, too, do many of my colleagues, who are starting to find overlapping evidence of such emergent order in all different kinds of complex systems.
The existence of spontaneous order is a stunning challenge to our settled ideas in biology since Darwin. Most biologists have believed for over a century that selection is the sole source of order in biology, that selection alone is the "tinkerer" that crafts the forms. But if the forms selection chooses among were generated by laws of complexity, then selection has always had a handmaiden. It is not, after all, the sole source of order, and organisms are not just tinkered-together contraptions, but expressions of deeper natural laws. If all this is true, what a revision of the Darwinian worldview will lie before us! Not we the accidental, but we the expected.
The revision of the Darwinian worldview will not be easy. Biologists have, as yet, no conceptual framework in which to study an evolutionary process that commingles both self-organization and selection. How does selection work on systems that already generate spontaneous order? Physics has its profound spontaneous order, but no need of selection. Biologists, subliminally aware of such spontaneous order, have nevertheless ignored it and focused almost entirely on selection. Without a framework to embrace both self-organization and selection, self-organization has been rendered almost invisible, like the background in
a gestalt picture. With a sudden visual shift, the background can become the foreground, and the former foreground, selection, can become the background. Neither alone suffices. Life and its evolution have always depended on the mutual embrace of spontaneous order and selection's crafting of that order. We need to paint a new picture.
Genesis
Two conceptual lineages from the nineteenth century linked to complete our sense of accidental isolation in the swirl of stars. In addition to Darwin, the science of thermodynamics and statistical mechanics, from the French engineer Sadi Carnot to the physicists Ludwig Boltzmann and Josiah Willard Gibbs, gave us the seemingly mysterious second law of thermodynamics: In equilibrium systems—those closed to the exchange of matter and energy with their environment—a measure of disorder called entropy inevitably increases. We have all seen the simple examples. A droplet of dark blue ink in a dish of still water diffuses to a uniform light blue. The ink does not reassemble into a single droplet.
Boltzmann gave us the modern understanding of the second law. Consider a box filled with gas molecules, modeled as hard elastic spheres. All the molecules could be in a small corner of the box, or they could be spread out more or less uniformly. One arrangement is just as unlikely as any other. But a vastly larger number of the possible configurations correspond to
    6

 situations in which the molecules are more or less uniformly distributed than to situations with all the molecules confined, say, to a single corner. Boltzmann argued that the increase in entropy in equilibrium systems arises from nothing more than the statistical tendency of the system to pass randomly through all possible arrangements (the so-called ergodic hypothesis). In the vast majority of these cases, the molecules will be distributed uniformly. And so, on average, that is what we will see. The ink droplet diffuses and does not reassemble; the gas molecules diffuse from one corner of the box and do not reassemble. Left to itself, a system will visit all possible microscopic, finegrained configurations equally often. But the system will spend most of its time in those coarse-grained patterns satisfied by very large numbers of fine-grained patterns—molecules uniformly distributed throughout the box. So the second law is not so mysterious, after all.
The consequence of the second law is that in equilibrium systems, order—the most unlikely of the arrangements—tends to disappear. If order is defined as those coarse-grained states that correspond to only a few fine-grained states (molecules bunched in the upper-left-hand cor-
ner, molecules arranged in a plane parallel to the top of the box), then at thermodynamic equilibrium, those delicate arrangements disappear because of the ergodic wandering of the system through all its mi-crostates. It follows that the maintenance of order requires that some form of work be done on the system. In the absence of work, order disappears. Hence we come to our current sense that an incoherent collapse of order is the natural state of things. Again, we the accidental, we the unexpected.
The second law of thermodynamics has been thought to be rather gloomy. One almost imagines the grave headlines: UNIVERSE RUNNING DOWN. HEAT DEATH HEADED OUR WAY. DISORDER IS ORDER OF THE DAY. How far we have come from the blessed children of God, at the center of the universe, walking among creatures created for our benefit, in a garden called Eden. Science, not sin, has indeed lost us our paradise.
If the universe is running down because of the second law, the easy evidence out my window is sparse—some litter here and there, and the heat given off by me, a homeotherm, scrambling the molecules of air. It is not entropy but the extraordinary surge toward order that strikes me. Trees grabbing sunlight from a star eight light-minutes away, swirling its photons together with mere water and carbon dioxide to cook up sugars and fancier carbohydrates; legumes sucking nitrogen from bacteria clinging to their roots to create proteins. I eagerly breathe the waste product of this photosynthesis, oxygen—the worst poison of the archaic world, when anaerobic bacteria ruled—and give off the carbon dioxide that feeds the trees. The biosphere around us sustains us, is created by us, grafts the energy flux from the sun into the great web of biochemical, biological, geologic, economic, and political exchanges that envelopes the world. Thermodynamics be damned. Genesis, thank whatever lord may be, has occurred. We all thrive.
The earliest signs of life on earth are present 3.45 billion years ago, 300 million years after the crust cooled sufficiently to support liquid water. These signs of life are hardly trivial. Well-formed cells, or what the experts believe are cells, are present in the archaic rocks from that period. Figure 1.1 shows such ancient fossils. Figure 1.2a shows contemporary coccoid cyanobacteria, while Figure 1.2b shows similar fossil cyanobacteria from 2.15 billion years ago. The morphological similarity is stunning. These ancient cells appear to have a cell membrane that separates an internal milieu from an external environment. Of course, morphological similarity does not demonstrate biochemical or metabolic similarity, but we look at these fossils with the chilling feeling that we are seeing imprints of the universal ancestor.
Cells were undoubtedly the triumphant culmination of some form of
    7

  Figure 1.1 The ancestors of us all: fossils, 3.437 billion years old. Each photograph is accompanied by an artist's rendition.
evolution from the earliest webs of interacting molecules complex enough to exhibit the qualities we call alive: the ability to metabolize, to reproduce, and to evolve. In turn, the origin of precellular life was itself the triumphant culmination of some form of prebiotic chemical evolution, which led from the limited diversity of molecular species present in the gas cloud of the protoearth to the enhanced chemical diversity that underwrote the crystallization of life, of self-reproducing molecular systems.
But once cells were invented, a numbing lassitude befell our earliest ancestors. Something like simple single-celled organisms, presumably related to current archaebacteria, and later, elongated tubelike cells that presumably were precursors of early fungi, seem to have formed a
a
   8

  b
Figure 1.2 Ancient fossil cells and modern living cells show a stunning resemblance, (a) Living coccoid cyanobacteria colony, (b) Fossil coccoid cyanobacteria colony, 2.15 billion years old, from Canada.
global co-prosperity sphere that persisted for perhaps 3 billion rather dull years. These progenitors of all later ecosystems formed complex local economies in which bacterial species and algae, competing and collaborating with one another, formed complex mound structures, typically several meters across and several meters high. Contemporary mounds are found in abundance along the Great Barrier Reef on the northeastern coast of Australia. Fossilized versions of these mounds are called stromatoliths. Presumably, these simple ecosystems covered the shallow coastal waters of the globe. Even today, the same type of formations are found in shallow waters in the Gulf of California and Australia. Contemporary mounds harbor hundreds of bacterial species and a modest number of algal species. One guesses that archaic mound systems were of a similar complexity.
Such single-celled life-forms persisted alone in the biosphere for perhaps 3 billion years—most of the age of the earth. But life-forms were destined to change by some unknown agency, some pregnancy of possibilities. Was it Darwinian chance and selection alone, as biologists have argued for decades? Or, as I shall propose in this book, did principles
of self-organization mingle with chance and necessity? By perhaps 800 million years ago, multicellular organisms appeared. The routes to the formation of such multicellular life remain obscure, although some workers believe that it occurred when a tubelike protofungi began to form internal walls that perhaps later became individual cells.
All hell broke loose in the Cambrian explosion, about 550 million years ago. A burst of evolutionary creativity generated almost all the major phyla now jostling one another in nooks and crannies on, above, under the earth's surface, everywhere, even thousands of feet down in solid rock. Only the vertebrates, our own lineage, arose a bit later, in Ordovician times.
The history of life in the first 100 million years following the Cambrian explosion was one of bustling confusion. And its mysteries are yet to be solved. The Linnean chart groups organisms hierarchically, from the specific to the general: species, genera, families, orders, classes, phyla, and kingdoms. One might imagine that the first multicellular creatures would all be very similar, only later diversifying, from the bottom up, into different genera, families, orders, classes, and so on. That, indeed, would be the expectation of the strictest conventional Darwinist. Darwin, profoundly influenced by the emerging view of geologic gradualism, proposed that all evolution occurred by the very gradual accumulation of useful variations. Thus the earliest multicellular creatures themselves ought to have diverged gradually from one another. But this appears to be false. One of the wonderful and puzzling features of the Cambrian explosion is that the chart was filled in from the top down. Nature suddenly sprang forth with many wildly different body plans—the phyla—elaborating on these basic designs to form the classes, orders, families, and genera.
In his book about the Cambrian explosion, Wonderful Life: The Burgess Shale and the Nature of History, Stephen Jay Gould remarks on this top-down quality of the Cambrian with wonder. In the Permian extinction, 245 million years ago, 96 percent of all species disappeared. But in the rebound, during which many new species evolved, diversity filled in from the bottom up, with many new families, a few new orders, one new class, and no new phyla.
Many are the arguments about this asymmetry between the Cambrian and the Permian explosions. My own view, explored in later chapters, is that the Cambrian explosion is like the earliest stages of the technological evolution of an entirely novel invention, such as the bicycle. Recall the funny early forms: big front wheels, little back ones; little front wheels, big back ones. A flurry of forms branched outward over Europe and the United States and elsewhere, giving rise to major and minor
    9

 variants. Soon after a major innovation, discovery of profoundly different variations is easy. Later innovation is limited to modest improvements on increasingly optimized designs.
So it was, I think, in the Cambrian, when multicellular life first tested out its possible modes of being. The particular branchings of life, were the tape played again, might differ, but the patterns of the branching, dramatic at first, then dwindling to twiddling with details later, are likely to be lawful. Biological evolution may be a deeply historical process, as Darwin taught us, but lawlike at the same time.
As we shall see in later chapters, the parallels between branching evolution in the tree of life and branching evolution in the tree of technology bespeak a common theme: both the evolution of complex organisms and the evolution of complex artifacts confront conflicting "design criteria." Heavier bones are stronger, but may make agile flight harder to achieve. Heavier beams are stronger, but may make agile fighter aircraft harder to achieve as well. Conflicting design criteria, in organism or artifact, create extremely difficult "optimization" problems—juggling acts in which the aim is to find the best array of compromises. In such problems, crudely designed major innovations can be greatly improved by dramatic variations on the new theme. Later on, as most of the major innovations have been tried, improvements dwindle to mere fiddling with the details. If something like this is true, then evolution's cadences may find echos in the evolution of artifacts and cultural forms we human tinkerers create.
The past 550 million years have witnessed well-fossilized life-forms emerging onto and then ebbing from the stage. Speciation and extinction go roughly hand in hand. Indeed, recent evidence suggests that the highest rate of extinction, as well as speciation, occurred in the Cambrian itself. Over the next 100 million years, the average diversity of species increased to a kind of rough steady state. But that level was, and persistently is, perturbed by small and large avalanches of extinctions that wipe out modest numbers or large numbers of species, genera, or families. Many of these catastrophes may have been caused by small and large meteors. Indeed, the extinction at the end of the Cretaceous, which coincided with the denouement of the dinosaurs, was probably caused by a massive misfortune that landed near the Yucatan.
In this book, I shall explore a different possibility. It does not always take a meteor or some outside cataclysm to wipe out whole species. Rather, speciation and extinction seem very likely to reflect the spontaneous dynamics of a community of species. The very struggle to survive, to adapt to the small and large changes of one's coevolutionary partners, may ultimately drive some species to extinction while creating novel
niches for others. Life, then, unrolls in an unending procession of change, with small and large bursts of speciations, and small and large bursts of extinctions, ringing out the old, ringing in the new. If this view is correct, then the patterns of life's bursts and burials are caused by internal processes, endogenous and natural. These patterns of speciations and extinctions, avalanching across ecosystems and time, are somehow self-organized, somehow collective emergent phenomena, somehow natural expressions of the laws of complexity we seek. And somehow, when understood, such patterns must afford us a deeper understanding of the game we have all joined, for we are all part of the same pageant.
No small matter these small and large avalanches of creativity and destruction, for the natural history of life for the past 550 million years has echoes of the same phenomena at all levels: from ecosystems to economic systems undergoing technological evolution, in which avalanches of new goods and technologies emerge and drive old ones extinct. Similar small and large avalanches even occur in evolving cultural systems. The natural history of life may harbor a new and unifying intellectual underpinning for our economic, cultural, and social life. I will spend much of this book unpacking the grounds to think that a deep theory for such ceaseless change can be found. I suspect that the fate of all complex adapting systems in the biosphere—from single cells to economies—is to evolve to a natural state between order and chaos, a grand compromise between structure and surprise. Here, at this poised state, small and large avalanches of coevolutionary change propagate through the system as a consequence of the small, best choices of the actors themselves, competing and cooperating to survive. I will suggest that, on small and large scales, we all do the best we can but will eventually be hustled offstage by some unanticipated consequences of our own best efforts. We will find a place in the sun, poised on the edge of chaos, sustained for a time in that sun's radiance, but only for a moment before we slip from sight. Untold many actors come and go, each, as a fine playwright once said, strutting and fretting its hour upon the stage. A smiling irony is our fate.
We all make our livings—frog, fern, bracken, bird, seafarer, or landed gentry. From the metabolic mutualisms of legume root and nitrogen-fixing bacteria, by which each makes a nutrient needed by the other, to the latest research partnership between drug giant and small biotech firm, we are all selling and trading our stuff to one another to get our daily bread. And somehow, the burgeoning diversity of the Cambrian— where each new species offers a novel niche or two to the others that feed on it, flee from it, or share with it—looks rather like the burgeoning diversity of an economic system in which each new good or service
  10

   affords a niche or two for other goods or services, whose providers thereby make a living. We are all trading our stuff to one another. We all must make our living. Might general laws govern all this activity? Might general laws govern phenomena ranging from the Cambrian explosion to our postmodern technological era, in which the exploding rate of innovation brings the time horizon of future shock ever closer? That is the possibility I will be exploring in this book.
The Laws of Life
Whence cometh all this activity, complexity, and exuberant chutzpah? If the physicists are right, all this diversity can only be understood as a consequence of the fundamental laws they have sought since Kepler and Galileo started becoming too advanced for the church. This most profound of scientific hopes, this search for fundamental laws, is to be held in the deepest respect: it is the ideal of reductionism in science. As Steven Weinberg phrased it in his recent book title, it is the "dream of a final theory. " Weinberg's characterization of the ancient quest is heartfelt. We seek reductionist explanations. Economic and social phenomena are to be explained in terms of human behavior. In turn, that behavior is to be explained in terms of biological processes, which are in turn to be explained by chemical processes, and they in turn by physical ones.
Much has been said about the validity of the reductionist program. But this much all of us would agree on: if we should find the final theory—perhaps superstrings embedded in 10-dimensional space with 6 dimensions curled in on themselves, and the remaining 4 whipped into some topological foam of quantized space-time, allowing gravity and the other three forces to fit into one conceptual framework—if we should find the final theory, we should have only begun our task. For on that genuinely magnificent day when the fundamental law should be graven forever on some Carrara marble stele or, as the physicist Leon Lederman suggests, on the front of a T-shirt, we should then truly have to begin to compute the consequences of that law.
Could we ever hope to carry out this second half of the reductionist program? Could we use the laws to understand the biosphere we see? We confront here the distinction between explaining and predicting. A table of the tides predicts but does not explain. Newton's theory predicts and explains. Many biologists think that Darwin's theory explains but only weakly predicts. Our final theory of physics might well explain, but almost certainly will not predict in detail. Failure to predict can it-
self already be predicted on at least two grounds. The first is quantum mechanics, which ensures a fundamental indeterminism at the subatomic level. Since that indeterminism has macroscopic consequences—for example, a random quantum event can cause a mutation in DNA molecules—we appear to be fundamentally forbidden from deriving detailed specific predictions about all molecular and supramolecular events. The second difficulty derives from the mathematical field now known as chaos theory. The central idea is simple and is captured in the so-called butterfly effect: a legendary butterfly flapping its wings in Rio changes the weather in Chicago. (I have lived in Chicago and personally suspect that nothing can change the weather there.) It always appears to be the same butterfly whenever anyone tells of this example. One would think it possible to imagine, by a vast conceptual leap, some other example: a moth in Omaha, perhaps, or a starling in Sheboygan. Whatever winged creature is responsible, the point is that any small change in a chaotic system can, and typically does, have large and amplifying effects. Thus this sensitivity implies that the detailed initial condition—how fast, at what angle, and precisely how the starling flapped its wings—would have to be known to infinite precision to predict the result. But both practical and quantum considerations preclude such a possibility. Thus the familiar conclusion: for chaotic systems, we cannot predict long-term behavior. Note again that failure to predict does not mean failure to understand or to explain. Indeed, if we were confident we knew the equations governing a chaotic system, we would be confident we understood its behavior, including our incapacity to predict in detail its long-term behavior.
If we will often, in general and in principle, be precluded from making detailed predictions with the final theory, then what can we hope for? I once listened with considerable interest to an interior decorator whose design sense was demonstrably better than my own. I learned a useful phrase: it's that kind of thing. Now here is a phrase that has quite cosmic usefulness, for even in our incapacity to predict details, we can still have every hope of predicting kinds of things. The hope here is to characterize classes of properties of systems that, in senses to be made more precise later, are typical or generic and do not depend on the details. For example, when water freezes, one does not know where every water molecule is, but a lot can be said about your typical lump of ice. It has a characteristic temperature, color, and hardness—"robust" or "generic" features that do not depend on the details of its construction. And so it might be with complex systems such as organisms and economies. Not knowing the details we nevertheless can build theories that seek to explain the generic properties.
    11

 Advances in theoretical science have often been based on finding useful compact descriptions of a phenomenon of interest. The reduced description does not capture all the features of the phenomenon, just those that are fundamentally relevant. A simple example is the pendulum of a grandfather dock, or, in fancier terms, a harmonic oscillator. The pendulum might be described in terms of composition, length, weight, color, engravings on its surface, distance from other objects, and so on. But to understand the fundamental property of periodic motion, length and mass are important; the rest are not. Statistical mechanics gives us our clearest example of the use of statistically averaged, hence typical and generic, properties as compact descriptors of a complex system. Temperature and pressure are averaged properties of a volume of gas at equilibrium that are typically insensitive to the detailed behaviors of individual gas molecules.
Statistical mechanics demonstrates that we can build theories about those properties of complex systems that are insensitive to the details. But the statistical mechanics of gases is relatively simple, for all the gas molecules obey the same Newtonian laws of motion, and we seek to understand the averaged collective motions of the gas molecules. Familiar statistical mechanics concerns simple random systems. Organisms are not simple random systems, but highly complex, heterogeneous systems that have evolved for almost 4 billion years. Discovering the existence of key biological properties of complex living systems that do not depend on all the details lies at the center of a hope to build a deep theory of biological order. If all properties of living systems depend on every detail of their structure and logic, if organisms are arbitrary widgets inside arbitrary contraptions all the way down, then the epistemological problems that confront us in attempting to understand the wonder of the biosphere will be vast. If, instead, core phenomena of the deepest importance do not depend on all the details, then we can hope to find beautiful and deep theories. For example, ontogeny, the development of a fertilized egg into an adult, is controlled by networks of genes and their products in each cell of the body. If this unfolding depends on every small detail of the network, then understanding the order in organisms would require knowing all those details. Instead, I shall give strong grounds in later chapters to think that much of the order seen in development arises almost without regard for how the networks of interacting genes are strung together. Such order is robust and emergent, a kind of collective crystallization of spontaneous structure. Here is order whose origin and character we can hope to explain independently of the details. Here is spontaneous order that selection then goes on to mold.
The search for such properties is emerging as a fundamental research strategy, one I shall make much use of in this book. In these cases, one hopes to explain, understand, and even predict the occurrence of these generic emergent properties; however, one gives up the dream of predicting the details. Examples that we shall explore include the origin of life as a collective emergent property of complex systems of chemicals, the development of the fertilized egg into the adult as an emergent property of complex networks of genes controlling one another's activities, and the behavior of coevolving species in ecosystems that generates small and large avalanches of extinction and speciation. In all these cases, the order that emerges depends on robust and typical properties of the systems, not on the details of structure and function. Under a vast range of different conditions, the order can barely help but express itself.
But how would such laws of emergent order, if they should someday be found, be reconciled with the random mutations and opportunistic selections of Darwinism? How can life be contingent, unpredictable, and accidental while obeying general laws? The same question arises when we think about history. Historians have differed, some eschewing any hope of finding general laws. I, surely no historian, shall nevertheless have suggestions to make. For the hope arises that viewed on the most general level, living systems—cells, organisms, economies, societies—may all exhibit lawlike properties, yet be graced with a lacework of historical filigree, those wonderful details that could easily have been otherwise, whose very unlikelihood elicits our awed admiration.
And so we return to the overarching question: Whence cometh all this bubbling activity, complexity, and chutzpah? It is our quest to understand the emergence of this ordered complexity around us, in the living forms we see, the ecosystems they construct, the social systems that abound from insects to primates, the wonder of economic systems that actually deliver us our daily bread and astonished Adam Smith into the conceptualization of his Invisible Hand. I am a doctor- biologist. I am hopeful enough to think I might possibly help in understanding the origin of life and its subsequent evolution. I am not a physicist. I am not brazen enough to presume to think about cosmic evolution. But I wonder: Whence cometh all this bubbling activity and complexity? Ultimately, it must be a natural expression of a universe that is not in equilibrium, where instead of the featureless homogeneity of a vessel of gas molecules, there are differences, potentials, that drive the formation of complexity. The flash of the Big Bang 15 billion years ago has yielded a universe said to be expanding, perhaps never to fall together in the Big Crunch. It is a nonequilibrium universe filled with too many hydrogen
and helium atoms compared with the most stable atomic form, iron. It is a universe of galaxies and clusters of galaxies on many scales, where none might have formed at all. It is a universe of stunningly abundant free energy available for performing work. The life around us must somehow be the natural consequence of the coupling of that free energy to forms
    12

 of matter. How? No one knows. But we shall hazard hypotheses along the way. Here is no mere scientific search. Here is a mystical longing, a sacred core first sought around that small campfire sometime in the past 3 million years. This way lies the search for our roots. If we are, in ways we do not yet see, natural expressions of matter and energy coupled together in nonequilibrium systems, if life in its abundance were bound to arise, not as an incalculably improbable accident, but as an expected fulûllment of the natural order, then we truly are at home in the universe.
Physicists, chemists, and biologists are familiar with two major forms by which order arises. The first involves what are known as low-energy equilibrium systems. A familiar example is a ball in a bowl that rolls to the bottom, wobbles a bit, and stops. The ball stops at a position that minimizes its potential energy. Its kinetic energy of motion, acquired because of gravity, has been dissipated into heat by friction. Once the ball is at equilibrium, located at the bottom of the bowl, no further input of energy is needed to maintain that spatial order. In biology, similar examples abound. Thus viruses are complex molecular systems of DNA or RNA molecular strands that form a core around which a variety of proteins assemble to form tail fibers, head structures, and other features. In an appropriate aqueous environment, the viral particle will self-assemble from its molecular DNA or RNA and protein constituents, seeking its state of lowest energy, like the ball in the bowl. Once the virus is formed, no further input of energy is required to maintain it.
The second means by which order arises requires a constant source of mass or energy or both to sustain the ordered structure. Unlike the ball in the bowl, such systems are nonequilibrium structures. A whirlpool in a bathtub is a familiar example. Once formed, the non-equilibrium swirl can be stable for long periods if water is continuously added to the tub and the drain is left open. One of the most startling examples of such a sustained nonequilibrium structure is the Great Red Spot on Jupiter, which appears to be a whirlpool in the upper atmosphere of that enormous planet. The Great Red Spot vortex, essentially a storm system, has been present for at least several centuries. Thus the lifetime of the Great Red Spot is far longer than the average time any single gas molecule has lingered within it. It is a stable organization of
matter and energy through which both matter and energy flow. The similarity to a human organism, whose molecular constituents change many times during a lifetime, is intriguing. One can have a remarkably complex discussion about whether the Great Red Spot might be considered to be living—and if not, why not. After all, the Great Red Spot in some sense persists and adapts to its environment, shedding baby vortices as it does so.
Nonequilibrium ordered systems like the Great Red Spot are sustained by the persistent dissipation of matter and energy, and so were named dissipative structures by the Nobel laureate Ilya Prigogine some decades ago. These systems have received enormous attention. In part, the interest lies in their contrast to equilibrium thermodynamic systems, where equilibrium is associated with collapse to the most probable, least ordered states. In dissipative systems, the flux of matter and energy through the system is a driving force generating order. In part, the interest lies in the awareness that free-living systems are dissipative structures, complex metabolic whirlpools. Here I am being careful to make a distinction between free-living systems and viruses. Viruses are not free-living entities; rather, they are parasites that must invade cells to reproduce. All known free-living systems are composed of cells, from bacteria to bottleflies. Cells are not low-energy structures. Cells hum along as complex chemical systems that persistently metabolize food molecules to maintain their internal structure and to reproduce. Hence cells are nonequilibrium dissipative structures. Interestingly, some simple cells such as spore forms, which presumably are low-energy structures, can enter nonmetabolizing quiescent states. For most cells, however, equilibrium corresponds to death.
Since all free-living systems are nonequilibrium systems—indeed, since the biosphere itself is a nonequilibrium system driven by the flux of solar radiation—it would be of the deepest importance were it possible to establish general laws predicting the behavior of all nonequilibrium systems. Unfortunately, efforts to find such laws have not yet met with success. Some believe that they may never be discovered. The failure may not be a lack of cleverness on our part, but a consequence of a well-founded area of mathematics called the theory of computation. This beautiful theory is concerned with what are called effectively computable algorithms. Algorithms are a set of procedures to generate the answer to a problem. An example is the algorithm to find the solution to a quadratic equation, which most of us were taught while learning algebra. Not only was I taught the algorithm, but our entire algebra class was invited to tattoo it on our tummies so that we could solve quadratic equations by rote. In short, algorithms can be carried out by any careful
dummy. Computers are just such dummies, and familiar computer programs are just algorithms.
The theory of computation is replete with deep theorems. Among the most beautiful are those showing that, in most cases by far, there exists no shorter means to predict what an algorithm will do than to simply execute it, observing the succession of actions and states as they unfold. The algorithm itself is its own shortest description. It is, in the jargon of the
    13

 field, incompressible.
The next step in the argument that no general laws could predict the detailed behavior of all nonequilibrium systems is simple. Real computers, made of real materials and plugged into a wall socket, are what Alan Turing called universal computational systems. He showed that with an infinitely long memory tape, a universal computer could carry out any algorithm at all. A physical computer qualifies as a nonequilibrium system; supplied with a constant source of energy, it is able to carry out calculations by using energy to maneuver electronic bits in silicon chips in various patterns. But the theory of computation tells us that such a device might be behaving in a way that is its own shortest description. The shortest way to predict what this real physical system will do is just to watch it. But the very purpose of a theory is to provide just such a shorter, compressed description—Kepler's laws instead of a catalog of every position of every planet at every moment in time. Since such a physical computer is a real nonequilibrium system, we could not have a general theory predicting the detailed behavior of all possible nonequilibrium systems. But cells, ecosystems, and economic systems are also real nonequilibrium systems. It is conceivable that these, too, behave in ways that are their own shortest descriptions.
In considering whether there can be laws of life, many biologists would answer with a firm no. Darwin has properly taught us of descent with modification. Modern biology sees itself as a deeply historical science. Shared features among organisms—the famous genetic code, the spinal column of the vertebrates—are seen not as expressions of underlying law, but as contingent useful accidents passed down through progeny as useful widgets, found and frozen thereafter into that descendant branch of life. It is by no means obvious that biology will yield laws beyond descent with modification. But I believe that such laws can be found.
We wish to understand the order around us in the biosphere, and we see that that order may reflect both low-energy equilibrium forms (the ball in the bowl, the virus) and nonequilibrium, dissipative structures, the living whirlpools that maintain order by importing and exporting matter and energy. Yet we have now encountered at least three difficul-
ties that stand in our way. First, quantum theory precludes detailed prediction of molecular phenomena. Whatever the final theory, this world has seen too many throws of the quantum dice to predict its detailed state. Second, even were classical determinism to hold, the theory of chaos shows us that very small changes in initial conditions can lead to profound changes of behavior in a chaotic system. As a matter of practicality, we may typically be unable to know initial conditions with sufficient precision to predict detailed behavior. Finally, the theory of computation seems to imply that nonequilibrium systems can be thought of as computers carrying out algorithms. For vast classes of such algorithms, no compact, lawlike description of their behavior can be obtained.
If the origin and evolution of life is like an incompressible computer algorithm, then, in principle, we can have no compact theory that predicts all the details of the unfolding. We must instead simply stand back and watch the pageant. I suspect that this intuition may prove correct. I suspect that evolution itself is deeply like an incompressible algorithm. If we demand to know its details, we must watch in awed wonder and count and recount the myriad rivulets of branching life and the multitudes of its molecular and morphological details.
And yet, even if it is true that evolution is such an incompressible process, it does not follow that we may not find deep and beautiful laws governing that unpredictable flow. For we are not precluded from the possibility that many features of organisms and their evolution are profoundly robust and insensitive to details. If, as I believe, many such robust properties exist, then deep and beautiful laws may govern the emergence of life and the population of the biosphere. After all, what we are after here is not necessarily detailed prediction, but explanation. We can never hope to predict the exact branchings of the tree of life, but we can uncover powerful laws that predict and explain their general shape. I hope for such laws. I even dare to hope that we can begin to sketch some of them now. For want of a better general phrase, I call these efforts a search for a theory of emergence.
Order for Free
The vast mystery of biology is that life should have emerged at all, that the order we see should have come to pass. A theory of emergence would account for the creation of the stunning order out our windows as a natural expression of some underlying laws. It would tell us if we are at home in the universe, expected in it, rather than present despite overwhelming odds.
Some words or phrases are evocative, even provocative. So it is with the word emergent. Commonly, we express this idea 14
    
 with the sentence, The whole is greater than the sum of its parts. The sentence is provocative, for what extra can be in the whole that is not in the parts? I believe that life itself is an emergent phenomenon, but I mean nothing mystical by this. In Chapters 2 and 3, I shall be at pains to give good reasons to believe that sufficiently complex mixes of chemicals can spontaneously crystallize into systems with the ability to collectively catalyze the network of chemical reactions by which the molecules themselves are formed. Such collectively autocatalytic sets sustain themselves and reproduce. This is no less than what we call a living metabolism, the tangle of chemical reactions that power every one of our cells. Life, in this view, is an emergent phenomenon arising as the molecular diversity of a prebiotic chemical system increases beyond a threshold of complexity. If true, then life is not located in the property of any single molecule— in the details—but is a collective property of systems of interacting molecules. Life, in this view, emerged whole and has always remained whole. Life, in this view, is not to be located in its parts, but in the collective emergent properties of the whole they create. Although life as an emergent phenomenon may be profound, its fundamental holism and emergence are not at all mysterious. A set of molecules either does or does not have the property that it is able to catalyze its own formation and reproduction from some simple food molecules. No vital force or extra substance is present in the emergent, self-reproducing whole. But the collective system does possess a stunning property not possessed by any of its parts. It is able to reproduce itself and to evolve. The collective system is alive. Its parts are just chemicals.
One of the most awesome aspects of biological order is ontogeny, the development of an adult organism. In humans, this process starts with a single cell, the fertilized egg, or zygote. The zygote undergoes about 50 cell divisions to create about 1 quadrillion cells that form the newborn infant. At the same time, the single cell type of the zygote differentiates to form the roughly 260 cell types of the adult—liver parenchymal cells, nerve cells, red blood cells, muscle cells, and so forth. The genetic instructions controlling development lie in the DNA within the nucleus of each cell. This genetic system harbors about 100,000 different genes, each encoding a different protein. Remarkably, the set of genes in all cell types is virtually identical. Cells differ because different subsets of genes are active within them, producing various enzymes and other proteins. Red blood cells have hemoglobin, muscle cells abound in the actin and myosin that form muscle fibers, and so forth. The magic of ontogeny lies in the fact that genes and their RNA and protein products
form a complex network, switching one another on and off in a wondrously precise manner.
We can think of this genomic system as a complex chemical computer, but this computer differs from familiar serial- processing computers, which carry out one action at a time. In the genomic computer system, many genes and their products are active at the same time; hence the system is a parallel-processing chemical computer of some kind. The different cell types of the developing embryo and its trajectory of development are, in some sense, expressions of the behavior of this complex genomic network. The network within each cell of any contemporary organism is the result of at least 1 billion years of evolution. Most biologists, heritors of the Darwinian tradition, suppose that the order of ontogeny is due to the grinding away of a molecular Rube Goldberg machine, slapped together piece by piece by evolution. I present a countering thesis: most of the beautiful order seen in ontogeny is spontaneous, a natural expression of the stunning self- organization that abounds in very complex regulatory networks. We appear to have been profoundly wrong. Order, vast and generative, arises naturally.
The emergent order seen in genomic networks foretells a conceptual struggle, perhaps even a conceptual revolution, in evolutionary theory. In this book, I propose that much of the order in organisms may not be the result of selection at all, but of the spontaneous order of self-organized systems. Order, vast and generative, not fought for against the entropic tides but freely available, undergirds all subsequent biological evolution. The order of organisms is natural, not merely the unexpected triumph of natural selection. For example, I shall later give strong grounds to think that the homeostatic stability of cells (the biological inertia that keeps a liver cell, say, from turning into a muscle cell), the number of cell types in an organism compared with the number of its genes, and other features are not chance results of Darwinian selection but part of the order for free afforded by the self-organization in genomic regulatory networks. If this idea is true, then we must rethink evolutionary theory, for the sources of order in the biosphere will now include both selection and self-organization.
This is a massive and difficult theme. We are just beginning to embrace it. In this new view of life, organisms are not merely tinkered-to-gether contraptions, bricolage, in Jacob's phrase. Evolution is not merely "chance caught on the wing," in Monod's evocative image. The history of life captures the natural order, on which selection is privileged to act. If this idea is true, many features of organisms are not merely historical accidents, but also reflections of the profound order that evolution has further molded. If true, we are at home in the uni-
verse in ways not imagined since Darwin stood natural theology on its head with his blind watchmaker. 15
    
 Yet more is presaged by self-organization. I said we must encompass the roles of both self-organization and Darwinian selection in evolution. But these sources of order may meld in complex ways that we hardly begin to understand. No theory in physics, chemistry, biology, or elsewhere has yet brokered this marriage. We must think anew. Among the progeny of this mating of self-organization and selection may be new universal laws.
It is perhaps astonishing, perhaps hopeful and wonderful, that we might even now begin to frame possible universal laws governing this proposed union. For what can the teeming molecules that hustled themselves into self-reproducing metabolisms, the cells coordinating their behaviors to form multicelled organisms, the ecosystems, and even economic and political systems have in common? The wonderful possibility, to be held as a working hypothesis, bold but fragile, is that on many fronts, life evolves toward a regime that is poised between order and chaos. The evocative phrase that points to this working hypothesis is this: life exists at the edge of chaos. Borrowing a metaphor from physics, life may exist near a kind of phase transition. Water exists in three phases: solid ice, liquid water, and gaseous steam. It now begins to appear that similar ideas might apply to complex adapting systems. For example, we will see that the genomic networks that control development from zygote to adult can exist in three major regimes: a frozen ordered regime, a gaseous chaotic regime, and a kind of liquid regime located in the region between order and chaos. It is a lovely hypothesis, with considerable supporting data, that genomic systems lie in the ordered regime near the phase transition to chaos. Were such systems too deeply into the frozen ordered regime, they would be too rigid to coordinate the complex sequences of genetic activities necessary for development. Were they too far into the gaseous chaotic regime, they would not be orderly enough. Networks in the regime near the edge of chaos—this compromise between order and surprise—appear best able to coordinate complex activities and best able to evolve as well. It is a very attractive hypothesis that natural selection achieves genetic regulatory networks that lie near the edge of chaos. Much of this book is bent on exploring this theme.
Evolution is a story of organisms adapting by genetic changes, seeking to improve their fitness. Biologists have long harbored images of fitness landscapes, where the peaks represent high fitness, and populations wander under the drives of mutation, selection, and random drift across the landscape seeking peaks, but perhaps never achieving them.
The idea of fitness peaks applies at many levels. For example, it can refer to the capacity of a protein molecule to catalyze a given chemical reaction. Then peaks of the landscape correspond to enzymes that are better catalysts for this reaction than all their neighboring proteins— those in the foothills and, worst of all, those in the valleys. Fitness peaks can also refer to the fitness of whole organisms. In that more complex case, an organism with a given set of traits is fitter—higher on the landscape—than all its near variants if, roughly speaking, it is more likely to have offspring.
We will find in this book that whether we are talking about organisms or economies, surprisingly general laws govern adaptive processes on multipeaked fitness landscapes. These general laws may account for phenomena ranging from the burst of the Cambrian explosion in biological evolution, where taxa fill in from the top down, to technological evolution, where striking variations arise early and dwindle to minor improvements. The edge-of-chaos theme also arises as a potential general law. In scaling the top of the fitness peaks, adapting populations that are too methodical and timid in their explorations are likely to get stuck in the foothills, thinking they have reached as high as they can go; but a search that is too wide ranging is also likely to fail. The best exploration of an evolutionary space occurs at a kind of phase transition between order and disorder, when populations begin to melt off the local peaks they have become fixated on and flow along ridges toward distant regions of higher fitness.
The edge-of-chaos image arises in coevolution as well, for as we evolve, so do our competitors; to remain fit, we must adapt to their adaptations. In coevolving systems, each partner clambers up its fitness landscape toward fitness peaks, even as that landscape is constantly deformed by the adaptive moves of its coevolutionary partners. Strikingly, such coevolving systems also behave in an ordered regime, a chaotic regime, and a transition regime. It is almost spooky that such systems seem to coevolve to the regime at the edge of chaos. As if by an invisible hand, each adapting species acts according to its own selfish advantage, yet the entire system appears magically to evolve to a poised state where, on average, each does as best as can be expected. Yet, as in many of the dynamical systems we will study in this book, each is eventually driven to extinction, despite its own best efforts, by the collective behavior of the system as a whole.
As we shall see, technological evolution may be governed by laws similar to those governing prebiotic chemical evolution and adaptive coevolution. The origin of life at a threshold of chemical diversity follows the same logic as a theory of economic takeoff at a threshold of di-
versity of goods and services. Above that critical diversity, new species of molecules, or goods and services, afford niches for yet further new species, which are awakened into existence in an explosion of possibilities. Like coevolutionary
    16

 systems, economic systems link the selfish activities of more or less myopic agents. Adaptive moves in biological evolution and technological evolution drive avalanches of speciation and extinction. In both cases, as if by an invisible hand, the system may tune itself to the poised edge of chaos where all players fare as well as possible, but ultimately exit the stage.
The edge of chaos may even provide a deep new understanding of the logic of democracy. We have enshrined democracy as our secular religion; we argue its moral and rational foundations, and base our lives on it. We hope that our heritage of democracy will spill out its abundance of freedom over the globe. And in the following chapters we will find surprising new grounds for the secular wisdom of democracy in its capacity to solve extremely hard problems characterized by intertwining webs of conflicting interests. People organize into communities, each of which acts for its own benefit, jockeying to seek compromises among conflicting interests. This seemingly haphazard process also shows an ordered regime where poor compromises are found quickly, a chaotic regime where no compromise is ever settled on, and a phase transition where compromises are achieved, but not quickly. The best compromises appear to occur at the phase transition between order and chaos. Thus we will see hints of an apologia for a pluralistic society as the natural design for adaptive compromise. Democracy may be far and away the best process to solve the complex problems of a complex evolving society, to find the peaks on the coevolutionary landscape where, on average, all have a chance to prosper.
Wisdom, Not Power
I suggest in the ensuing chapters how life may have formed as a natural consequence of physics and chemistry, how the molecular complexity of the biosphere burgeoned along a boundary between order and chaos, how the order of ontogeny may be natural, and how general laws about the edge of chaos may govern coevolving communities of species, of technologies, and even of ideologies.
This poised edge of chaos is a remarkable place. It is a close cousin of recent remarkable findings in a theory physicists Per Bak, Chao Tang, and Kurt Wiesenfeld called self-organized criticality. The central image
here is of a sandpile on a table onto which sand is added at a constant slow rate. Eventually, the sand piles up and avalanches begin. What one finds are lots of small avalanches and few large ones. If the size of the avalanche is plotted on the familiar x-axis of a Cartesian coordinate system, and the number of avalanches at that size are plotted on the y-axis, a curve is obtained. The result is a relationship called a power law. The particular shape of this curve, to which we shall return in later chapters, has the stunning implication that the same-sized grain of sand can unleash small or large avalanches. Although we can say that in general there will be more tiny avalanches and only a few big landslides (that is the nature of a power-law distribution), there is no way to tell whether a particular one will be insignificant or catastrophic.
Sandpiles, self-organized criticality, and the edge of chaos. If I am right, the very nature of coevolution is to attain this edge of chaos, a web of compromises where each species prospers as well as possible but where none can be sure if its best next step will set off a trickle or a landslide. In this precarious world, avalanches, small and large, sweep the system relentlessly. One's own footsteps shed small and large avalanches, which sweep up or by the other hikers on the slopes below. One may even be carried off in the avalanche started by his or her own footsteps. This image may capture the essential features of the new theory of emergence we seek. At this poised state between order and chaos, the players cannot foretell the unfolding consequences of their actions. While there is law in the distribution of avalanche sizes that arise in the poised state, there is unpredictability in each individual case. If one can never know if the next footstep is the one that will unleash the landslide of the century, then it pays to tread carefully.
In such a poised world, we must give up the pretense of long-term prediction. We cannot know the true consequences of our own best actions. All we players can do is be locally wise, not globally wise. All we can do, all anyone can do, is hitch up our pants, put on our galoshes, and get on with it the best we can. Only God has the wisdom to understand the final law, the throws of the quantum dice. Only God can foretell the future. We, myopic after 3.45 billion years of design, cannot. We, with all the others, cannot foretell the avalanches and their inter-twinings that we jointly generate. We can do only our local, level best. We can get on with it.
Since the time of Bacon our Western tradition has regarded knowledge as power. But as the scale of our activities in space and time has increased, we are being driven to understand the limited scope of our understanding and even our potential understanding. If we find general
    17

 laws, and if those laws entail that the biosphere and all within it co-evolve to some analogue of the sandpile, poised on the edge of chaos, it would be wise to be wise. We enter a new millennium. It is best to do so with gentle reverence for the ever-changing and unpredictable places in the sun that we craft ever anew for one another. We are all at home in the universe, poised to sanctify by our best, brief, only stay.
Chapter 2
The Origins of Life
Anyone who tells you that he or she knows how life started on the sere earth some 3.45 billion years ago is a fool or a knave. Nobody knows. Indeed, we may never recover the actual historical sequence of molecular events that led to the first self-reproducing, evolving molecular systems to flower forth more than 3 million millennia ago. But if the historical pathway should forever remain hidden, we can still develop bodies of theory and experiment to show how life might realistically have crystallized, rooted, then covered our globe. Yet the caveat: nobody knows.
First was the word, then the cleaving of darkness from light. By the third day, life-forms were crafted: fish, fowl, and more. Adam and Eve awakened on the sixth day. Not so wrong, after all, is this mythos in which life was believed to arise so rapidly. In reality, life sprang from the molten earth's womb about as soon as the infall of meteoric masses forming the protoearth slowed substantially and the surface cooled enough to support liquid water, an arena in which chemicals could combine to produce metabolisms. The earth is about 4 billion years old. No one knows what the earliest self-reproducing molecular systems looked like. But by 3.45 billion years ago, archaic forms of cells fingered some clay or rock surfaces, were buried, and left their traces for our later questions. I am no expert on such ancient fossils, but was delighted in Chapter 1 to share the beautiful work of William Schopf and his colleagues around the world. Figures 1.1 and 1.2 show some of the earliest fossil cells.
What prodigious progress these early cells exhibit! One guesses from their morphology that, like contemporary cells, their membrane was a bilipid layer—a kind of double-layered soap bubble made of fatty lipid molecules—enclosing a network of molecules that had the power to
sustain itself and reproduce. But how could such self-reproducing assemblages of molecular effort have coagulated from the primeval cloud of hydrogen and larger atoms and molecules, which itself coagulated from a dust cloud into the early earth? We stand, with Homo habilis, in need of a creation myth. This time, armed with the power of late-twentieth-century science, perhaps we can stumble on the truth.
Theories of Life
The question of the origin of life has undergone major transformations over the past centuries. This is not so surprising. In the Western tradition a thousand years ago, most who thought of the problem were persuaded that life formed spontaneously from nonlife. After all, maggots seemed to appear from nothing, in fruits and rotting wood, and fullblown adult insects hastened forth from their metamorphic chrysalis tents. Life sprang unbidden from moldering places, damp with promise. Spontaneous generation was but another miracle of the daily sort that God's hand shaped—quotidian, common, sustaining.
Theories of the origin of life began to appear in their modern forms only with the brilliant experiments of Louis Pasteur over a century ago. How could one mind have done so much? A prize was offered for the most telling experiments concerning the theory of spontaneous generation. The growth of bacterial populations had been demonstrated within what were believed to be initially sterile solutions. Pasteur rightly suspected that the source of the bacteria was the air itself, for the flasks used by his predecessors were open and crafted in such a way that it would be easy for bacteria to drift into the broth. Pasteur set about creating flasks with swan-necked, S-shaped openings. He hoped that any bacteria that entered from the outside would be trapped before they could reach the broth. Simple, elegant experiments always delight us the most. Pasteur found no growth of bacteria in his sterile broth. Life, he concluded, came from life.
But if life comes only from life, then whence life in the first place? With Pasteur, the problem of origins looms suddenly vast, profound, mysterious, perhaps unstatable, perhaps beyond science itself. Alchemy had led to chemistry, which led to the analysis of inorganic atoms and molecules: lead, copper salts, gold, oxygen, hydrogen. But organisms harbored
    18

 molecules not found in nonliving materials. Organisms harbored organic molecules. The difference between living and nonliving was, for some time, supposed to reside in these different kinds of molecules. No bridge could close the gap. Then, midway through the nine-
teenth century, Emil Fischer synthesized urea, clearly an organic compound, from inorganic chemicals. Life was made of the same stuff as nonlife. Fischer's result carried the implication that common physical and chemical principles might govern both living and nonliving matter. His achievement remains a major step in the reduction of biology to chemistry and physics. In a way, the believers in spontaneous generation were right after all: life did come from nonlife, though this conjuring act was far more complex than they ever could have supposed.
But the reductionist thesis that life is based on the same principles as nonlife was not to be so readily accepted. For even if it is granted that life is cut from the same cloth as nonlife, it does not follow that the cloth by itself suffices. The clothes need not, in truth, quite make the man. The French philosopher Henri Bergson proposed an answer to this wonderful mystery that persuaded many for decades: élan vital. Like any other fine French perfume, without which flesh was, well, merely flesh, élan vital was said to be an insubstantial essence that permeated and animated the inorganic molecules of cells and brought them to life. Was this really so silly? It is easy to be smug until our own cherished certitudes crumble. After all, frog muscles had recently been shown to exhibit animal magnetism—now more properly understood as the electric potential changes that propagate along nerve and muscle fibers—and the magnetic field of James Clerk Maxwell was itself insubstantial yet able to move matter held in its sway. If an insubstantial magnetic field could move solid matter, why could not an insubstantial élan vital animate the inanimate?
Bergson was not the only thoughtful person to advance such vitalist ideas. Hans Dreisch, a brilliant experimentalist, came to much the same conclusions. Dreisch had performed experiments on the two-celled frog embryo. Like most other embryos, the fertilized cell, or zygote, divides again and again, creating 2, 4, 8, 16 cells, and on and on until a creature is born. Dreisch looped a blond hair of a child around the embryo, pinching its two cells free from each other. To his utter astonishment, each cell developed into a fully normal frog! Even single cells isolated from later embryos, consisting of four and eight cells, could give rise to entire adult frogs.
Dreisch was no fool. He realized he had a hell of a puzzle on his hands. Nothing obvious in the entire Newtonian tradition of physical and chemical science appeared to offer even a glimmer of hope to explain such an astonishing result. It would have been acceptable if each part of the embryo gave rise to one part of an adult; indeed, this occurs in the embryos of many species and is called mosaic development. Mosaic development might be understood using the arguments espoused
by a group called the preformationists. The egg supposedly housed a homunculus, a tiny version of the adult, each part of which somehow expanded into the corresponding part of the adult. Thus deletion of half the egg—one of the two daughter cells of the zygote—would be expected to delete half the homunculus. The remaining half-egg or single cell should yield half a frog. But that is not what happened. Even if it had, the preformationists would still have been left with the overwhelming problem of explaining how the newly formed adult gave birth to a child that grew to an adult and gave birth to another child, and so on down the genealogical line. The preformationists suggested that the problem could be solved by supposing that, inside the egg, homunculi were nested inside homunculi like Chinese dolls—all the way back to creation. If life were to persist forever, of course, an infinity of such nested homunculi would be needed. Here, I confess, my willingness to sympathize with out-of-date ideas fades away. Theories, even those that prove incorrect, can have elegance and beauty, or be utterly ad hoc. A theory requiring an infinite series of ever smaller homunculi is too ad hoc to be true.
Dreisch had made an important discovery. If each cell in the two- or four- or eight-celled embryo could give rise to an entire adult, then the information had to come from somewhere. Somehow, the order was emergent; each part could give rise to the whole. But where did the information come from within each part? Dreisch turned to what were called entelechies, nonmaterial sources of order that invested the embryo and its mere stuff, and somehow led to the capacity of each part to give rise, magically, to the whole.
The origin-of-life problem rested quietly from the late nineteenth century for some 50 or more years, held by most thinkers to be either unapproachable scientifically or, at best, so premature that any efforts were hopeless. In the middle of the twentieth century, attention turned to the nature of the primitive earthly atmosphere that gave rise to the chemicals of life. Good evidence, now subject to some doubt, suggested that the early atmosphere was rich in molecular species such as hydrogen, methane, and carbon dioxide. Almost no oxygen was present. Further, it was supposed, simple organic molecules in the atmosphere, along with other more complex ones, would be expected to dissolve slowly in the newly formed oceans, creating a prebiotic soup. From this soup, it was hoped, life would somehow form spontaneously.
    19

 This hypothesis continues to have many adherents, though it suffers from considerable difficulties. Chief among them is the fact that the soup would be extremely dilute. The rate of chemical reactions depends on how rapidly the reacting molecular species encounter one another—
and that depends on how high their concentrations are. If the concentration of each is low, the chance that they will collide is very much lower. In a dilute prebiotic soup, reactions would be very slow indeed. A wonderful cartoon I recently saw captures this. It was entitled "The Origin of Life." Dateline 3.874 billion years ago. Two amino acids drift close together at the base of a bleak rocky cliff; three seconds later, the two amino acids drift apart. About 4.12 million years later, two amino acids drift close to each other at the base of a primeval cliff. . . . Well, Rome wasn't built in a day. Could life have crystallized in such a dilute medium—even if we waited as long as the age of the universe? We return in a moment to unhappy calculations, calculations that I find faulty but fun, which suggest that life could not have crystallized by chance in billions of times the lifetime of the universe. How unfortunate, for here I sit writing for you to read. Something must be wrong somewhere.
Alexander Oparin, a Russian biophysicist, writing from the hellhole of Stalinist life, proposed a plausible way to confront the problem afforded by the dilute soup. When glycerine is mixed with other molecules, it forms gel-like structures called coascervates. The coascervate is able to concentrate organic molecules inside itself and exchange them across its boundary. In short, coascervates are like primitive cells, which separate the molecular activities within from the dilute aqueous soup. If these tiny compartments had developed in the primeval waters, they might have concentrated the proper chemicals needed to form metabolisms.
If Oparin opened the door to understanding how protocells might have formed, it remained entirely unclear where their contents—the small organic molecules whose traffic is metabolism—came from. In addition to simple molecules are various polymers, long molecular chains of nearly identical building blocks. Proteins, of which muscles, enzymes, and the scaffolding of cells are made, consist of chains of 20 kinds of amino acids. This linear primary structure then folds up into a more or less compact three-dimensional structure. DNA and RNA are composed of chains of four nucleotide building blocks: adenine, cyto-sine, guanine, and thymine in DNA, with uracil substituting for thymine in RNA. Without these molecules, the very stuff of life, Oparin's coascervates would be no more than empty shells. Where, then, might these building blocks have come from?
In 1952, Stanley Miller, then a young graduate student in the laboratory of a famous chemist named Harold Urey, tried a dream of a crazy idea. He filled a flask with the gases—methane, carbon dioxide, and so forth—that were generally assumed to have been present in the atmosphere of the primitive earth. He showered the flask with sparks, mim-
icking lightning as a source of energy. He waited, hoping that he might be peering into a homemade Garden of Eden. Several days later, he was rewarded with evidence of molecular creativity: brown gunk clung to the sides and bottom of his flask. Upon analysis, this tarry material proved to contain a rich variety of amino acids. Miller had performed the first prebiotic chemistry experiment. He had discovered plausible means whereby the building blocks of proteins might have been formed on the early earth. He received his Ph.D. and has been a leader in the field of prebiotic chemistry ever since.
Similar experiments have shown that it is possible (though with much greater difficulty) to form the nucleotide building blocks of DNA, RNA, and fatty molecules and hence, through them, the structural material for cellular membranes. Many other small molecular components of organisms have been synthesized abiogenically.
But substantial puzzles remain. Robert Shapiro notes in his book Origins: A Skeptic's Guide to the Creation of Life on Earth that even though scientists can show that it is possible to synthesize the various ingredients of life, it is not easy to get them to cohere into a single story. One group of scientists discovers that molecule A can be formed from molecules  and  in a very low yield under a certain set of conditions. Then, having shown that it is possible to make A, another group starts with a high concentration of the molecule and shows that by adding D one can form E—again in a very low yield and under quite different conditions. Then another group shows that E, in high concentration, can form F under still different conditions. But how, without supervision, did all the building blocks come together at high enough concentrations in one place and at one time to get a metabolism going? Too many scene changes in this theater, argues Shapiro, with no stage manager.
The discovery of the molecular structure of genes, the famous DNA double helix, is the final event that underlies the resurging interest in the origin of life. Before James Watson and Francis Crick's famous paper in 1953, it was a matter of
    20

 deep debate among biologists and biochemists whether the genetic material would prove to be protein or DNA. Those favoring proteins as the fundamental genetic material had much to say in favor of their hypothesis: most notably, almost all enzymes are proteins. Enzymes, of course, are the major class of biological catalysts, molecules that bind to substrates and speed up the rate of the reactions necessary to make a metabolism. In addition, many of the structural molecules in cells are proteins. A familiar example is hemoglobin, found in red blood cells and important in binding and transporting oxygen from the lungs to the tissues. Since proteins are ubiquitous and are the workhorses of the body's cellular scaffolding and
metabolic flux, it was not unreasonable to suppose that these complex polymers of amino acids were also the carriers of genetic information.
An intellectual lineage beginning with Mendel, however, pointed to the chromosomes found in each cell as the carriers of genetic information. Most readers will be familiar with Mendel's beautiful genetic experiments on sweet peas in the 1870s. Atomism was the intellectual order of the day, for the burgeoning science of chemistry had found strong reasons to suppose that chemical reactions formed molecules out of simple whole-number ratios of their atomic building blocks. Water is precisely H20—two hydrogens (never two and a half) and one oxygen.
If atoms underlie chemistry, might there not be atoms of heredity? Children look something like each parent. Suppose this were caused by atoms of heredity, some from the mother and some from the father. But parents have parents, back for vast numbers of generations. If all the atoms of heredity were passed from each parent to each offspring, an enormous number of them would accumulate. To prevent this, on average, each offspring should receive only half the heredity atoms from each parent. The simplest hypothesis is that each offspring receives exactly one atom of heredity per trait from each parent. The two atoms would determine the trait—blue eyes versus brown eyes, for example— and would be passed on in turn to the next generation.
The rediscovery of Mendel's laws in 1902—nobody paid any attention to his initial discovery—is one of biology's heart- warming stories. Chromosomes, so named because of the colored stain that allowed them to show up under a microscope, had been identified in the nuclei of plant and animal cells. At cell division, or mitosis, the nucleus also divides. Each chromosome within the nucleus is first duplicated, and then one copy passes to each daughter nucleus, hence to each daughter cell. But even more impressive is the cellular process called meiosis, by which sperm and egg are formed. In meiosis, the number of chromosomes that reach the sperm or egg is exacdy half that which is in the other cells of the body. Only when egg combines with sperm to produce a zygote is the full genetic inheritance restored. Each regular cell in the body, called a somatic cell, has pairs of chromosomes: one from the father, one from the mother. Further work showed that when egg or sperm cells are formed, either the maternal or the paternal chromosome of each pair is chosen at random and passed along. Since Mendel's laws demand that each parent pass a randomly chosen half of its genetic instructions to the offspring, the conclusion was hardly escapable: the chromosomes must be the carriers of genetic information. The flowering of experimental genetics had, by the 1940s, afforded overwhelming confirmation to this belief.
But the chromosomes are made primarily of a complex polymer called DNA, or dioxyribonucleic acid. Thus it seemed likely that genes, the new name for atoms of heredity, might be made from DNA. A famous experiment by microbiologist Oswald Avery settled the issue. Avery coaxed bacteria to take up some pure DNA derived from other bacteria. The recipient bacteria then exhibited some traits of the donor bacteria, and the new trait was stably inherited when the bacteria divided. DNA could carry heritable genetic information.
The race was on to discover what it is about DNA that allows it to encode this information. The story of the double helix, with its complementary strands, is famous. DNA, heralded as the master molecule of life—a view with which I both agree and profoundly disagree—proved to be a double helix of four nucleotide bases: adenine (A), guanine (G), cytosine (C), and thymine (T). As most readers know, the magic lies in the specific base pairing: A bonds specifically to T;  bonds specifically to G. The genetic information is carried in the sequence of bases along one or the other strand of the double helix. Triplets of bases—AAA, GCA, and so forth—specify each amino acid. Thus the sequence of bases can be translated by the cell into a specific sequence of amino acids to form a precise protein.
It is hard not to marvel at how the double-helix structure of DNA immediately suggested how the molecule might replicate. Each strand specifies the nucleotide sequence of the complementary strand by the precise A-T and C-G base pairing. Knowing the sequence along one strand, call it Watson, tells one what the sequence along the other strand, Crick, must be.
If DNA is a double helix in which each strand is the complement of the other, if the sequence of bases along Watson 21
    
 specifies the sequence of bases along Crick and vice versa, then the DNA double helix might be a molecule able to replicate itself spontaneously. DNA, in short, becomes a candidate for the first living molecule. The very molecule that is hailed as the master molecule of present life, the carrier of the genetic program by which the organism is computed from the fertilized egg, the very same magical molecule might have been the first self-reproducing molecule at the dawn of life. It would multiply, eventually chancing on the recipe to make proteins to clothe itself and to speed its reactions by catalyzing them.
Those who wished to believe that life began with nucleic acids, however, were faced with an inconvenient fact: nude DNA does not self-replicate. A complex assemblage of protein enzymes must already be in place. Subsequent work by biochemists Matthew Messelson and Franklin Stahl showed that DNA in the chromosomes within cells does
replicate much as its structure suggests. Watson does specify a new-Crick; Crick does specify a new Watson. But the cellular dance is mediated by a host of protein enzymes.
Those seeking the first living molecule would have to look elsewhere. And soon another polymer hove into the biologists' sight. RNA, or ribonucleic acid, is the first cousin of DNA and is central to the functioning of a cell. Like DNA, RNA is a polymer of four nucleotide bases: A, C, and G, as in DNA, but with uracil, U, substituting for thymine. RNA can exist as a single-stranded form or as a double helix. Like DNA, the two strands of the double helix of RNA are template complements. In the cell, the information to make a protein is copied from the DNA to a strand of so-called messenger RNA and is ferried to structures called ribosomes. At these sites, with the help of another kind of RNA molecule, transfer RNA, protein is fabricated.
The template complementarity of double-stranded RNA suggested to many scientists that RNA might be capable of replicating itself without the help of protein enzymes. Thus life might have begun as proliferating molecules of RNA— nude genes, as they are sometimes called. Perhaps sadly, the efforts to get RNA strands to copy themselves in a test tube have failed. But the idea is simple and beautiful: place in a beaker a high concentration of a specific single-stranded sequence—say, the de-canucleotide CCCCCCCCCC. In addition, place a high concentration of free G nucleotides. Each G should line up with one of the  nucleotides in the decanucleotide, by virtue of Watson-Crick base pairing, such that a set of 10 G monomers are lined up adjacent to one another. It only remains that the 10 G monomer nucleotides become joined to one another by the proper bonds. Then what molecular biologists call a polyG decamer would have been formed. Then the two strands, polyG and polyC, need only melt apart, leaving the initial polyC decamer free to line up 10 more G monomers and create yet another polyG decamer. And finally, of course, to obtain a reproducing system of molecules, one hopes that the newly created polyG decanters, GGGGGGGGGG, might in turn be able to line up any free  monomers added to the beaker and cause the free  monomers to link together to form a polyC decamer, CCCCCCCCCC. Were all that to occur, and do so in the absence of any enzymes, then such a double-stranded RNA molecule would indeed be a nude replicating RNA molecule. Such a molecule would be a powerful candidate to be the first living molecule.
The idea is crisp and lovely. Almost without exception, however, the experiment does not work. The ways it fails are very instructive. First, each of the four nucleotides has its own chemical personality, which
tends to make the experiment fail. Thus single-stranded polyG has a tendency to curl back on itself in a hairpin so that two G nucleotides bond to each other. The result is a tangled mess that is incapable of acting as a template for self-copying. Starting with a sequence of  and G monomers, richer in  than in G, a complementary string can be easily created. But that complement is necessarily richer in G than in C, and thus tends to curl back on itself, removing itself from the game. Watson makes Crick. Crick examines his navel and refuses to play.
Even if copying were not brought to a halt by guanine tangles, naked RNA molecules could suffer from what is called an error catastrophe: in copying one strand into another, misplaced bases—a G where   should be—will corrupt the genetic message. In cells, these mistakes are kept to a minimum by proofreading enzymes that ensure faithful copies. Those rare mistakes that slip through the net are the mutations that drive evolution; most are detrimental, but occasionally one nudges the organism into a slightly fitter state. But without enzymes to avoid guanine tangles, copying errors, and other mistakes, an RNA message on its own could quickly become nonsense. And where, in a world of pure RNA, would the enzymes come from?
Some of those who believe that life began with RNA seek ways around this problem. Perhaps, they argue, a simpler self- replicating molecule came before RNA, one that was not plagued with guanine tangles and other problems. No clear
    22

 experimental work currently backs up this approach. Should it work, we would also confront the question of how evolution converted such simpler polymers to RNA and DNA.
If enzymes are absolutely necessary to replication, then those who believe that RNA came first—and this is the mainstream view—must seek ways that nucleic acids themselves can act as catalysts. A mere decade ago, most biologists, chemists, and molecular biologists held to the view that the catalytic molecules of the cell were exclusively protein enzymes and that both DNA and RNA were essentially chemically inert storehouses of information. A faint whiff that RNA might be of more dynamic importance might have been scented in the fact that special RNA molecules, called transfer RNAs, play a critical role that can hardly be considered passive in translating the genetic code into proteins. Furthermore, the ribosome, the molecular machine in the cell that accomplishes the translation, is made up largely of RNA sequences plus some proteins. This machinery is almost identical throughout the living world and thus probably existed almost from the beginning of the time when matter became alive. But it was not until the mid-1980s that Thomas Cech and his colleagues made the stunning discovery that RNA molecules themselves can act as enzymes and catalyze reactions. Such RNA sequences are called ribozymes.
When the DNA message—the instructions for making a protein—is copied onto a strand of messenger RNA, a certain amount of information is ignored. And so cells have not only proofreading enzymes, but editing enzymes. The part of the sequence that contains the genetic instructions, the exons, must be separated from the nonsense sections, the introns. And so enzymes are used to snip out the introns from the RNA and splice together the exons. The sequence of now adjacent exons is processed in still other ways, is transported from the nucleus, finds a ri-bosome, and is translated into a protein. Cech discovered, undoubtedly with some astonishment, that in some cases no protein enzyme is needed for the editing. The RNA sequence itself acts as an enzyme, cutting out its own introns. The results rather flabbergasted the molecular biology community. It now turns out that a variety of such ribozymes exist and can catalyze a variety of reactions, acting on themselves or on other RNA sequences. For example, one is able to transfer   nucleotide from the end of one sequence to the end of another: (CCCC) + (CCCC) yields (CCC) + (CCCCC).
RNA molecules, in the absence of protein enzymes, appear rather gauche at self-reproduction. But perhaps an RNA ribozyme might act as an enzyme and catalyze the reproduction of RNA molecules. And perhaps such a ribozyme might act on itself to reproduce itself. Either way, a self-reproducing molecule, or system of molecules, would lie to hand. Life would be under way.
It is important to be clear what is being requested of this ribozyme. Genetic information is carried in the sequence of nucleotide bases. Thus CCC is different from UAG. If one strand of an RNA molecule is UAGGCCUAAUUGA, then as its complementary strand is synthesized, the growing new sequence should be AUCCGGAUUAACU. As each new nucleotide is added, the proper choice must be made among the four possible nucleotides, and the proper bond must be formed. Protein enzymes are able ta accomplish this fine discrimination and are called polymerases. RNA and DNA polymerases are utterly essential to the synthesis of RNA and DNA sequences within the cell. But it may not be easy to find an RNA sequence able to perform this polymerase function. Nevertheless, such a ribozyme polymerase is fully plausible. Such a molecule may have been present at the dawn of life.
And, again, maybe it was not. Indeed, a serious problem assails the ribozyme polymerase hypothesis. Grant that such a fine molecule arose. Could it sustain itself against mutational degradation? And could it evolve? The answer to both questions seems likely to be no. The problem is a form of an error catastrophe, first described by the chemist Leslie Orgel in the context of the genetic code. Picture a ribozyme that is able to function as a polymerase and copy any RNA molecule, includ-
ing itself. Given a supply of nucleotides, this ribozyme would constitute a nude replicating gene. But any enzyme only hastens the correct reaction among the alternative possible side reactions that might also occur. Errors are inevitable. The self-reproducing ribozyme would necessarily produce mutant variants. But those mutant variant ribozymes themselves are likely to be less efficient than the normal, or wild-type, ribozyme, and hence are likely to make errors more frequently. These sloppier ribozymes will tend to reproduce themselves with even more mutants per copy than the wild-type ribozyme. Worse, the sloppy mutant ribozymes are able to catalyze the reproduction of the wild-type ribozyme, creating still more mutants. Over cycles, the system could produce a runaway spectrum of mutant variants. If so, the original ribozyme, with its ability to faithfully copy itself and others, could be lost in a flurry of sloppy catalysis leading to a system of RNA sequences that are catalytically inert. Life would have vanished in a runaway error catastrophe. I do not know of a detailed analysis of this specific problem, but I think that the potential for an error catastrophe for such a self-reproducing ribozyme deserves real analysis and encourages some caution about an otherwise very attractive hypothesis.
    23

 Of all the problems with the hypothesis that life started as nude replicating RNA molecules, the one I find most insurmountable is the one most rarely talked about: all living things seem to have a minimal complexity below which it is impossible to go. The simplest free-living cells are called pleuromona, a highly simplified kind of bacterium, replete with cell membrane, genes, RNA, protein-synthesizing machinery, proteins—the full complement of standard gear. The number of genes in pleuromona is variously estimated at a few hundred to about a thousand, compared with the estimated 3,000 in Escherichia coli, a bacterium in our intestines. Pleuromona is the simplest thing that we know to be alive. Your curiosity should be aroused. Viruses, which are vastly simpler than the pleuromona, are not free living. They are parasites that invade cells, co-opt the cell's metabolic machinery to accomplish their own self-reproduction, escape the host cell, and invade another. All free-living cells have at least the minimum molecular diversity of pleuromona. Your antenna should quiver a bit here. Why is there this minimal complexity? Why can't a system simpler than pleuromona be alive?
The best answer that the advocates of an RNA world can offer is an evolutionary just-so story, in honor of Rudyard Kipling and his fanciful tales about how different animals came to be. In medical school, I learned that a bone filled with small holes, fenestrated, and called the cribiform plate, forms the junction of the nose and forehead. The evolutionary rationale of this bone was explained to us: light and strong, well adapted to its function. Now, had the cribiform plate been a solid
chunk of knobby bone creating a horny protrusion, a kind of awning to my proboscis, I have no doubt that our professor would again have found a use for this solid massive knob, highly adapted for banging one's head against the wall. Evolution is filled with these just-so stories, plausible scenarios for which no evidence can be found, stories we love to tell but on which we should place no intellectual reliance.
What would Kipling (or, for that matter, most evolutionary biologists) say about why simple RNA replicators gave rise to a world where life seems to occur only above a threshold of complexity? Because, comes the answer, these first living molecules, driven by mutation and the survival of the fittest, gathered about themselves the vestments of metabolism, membrane, and so forth. Eventually, they evolved into the cells we have today. Fully clothed, the current minimal cell happens to have the observed minimal complexity. But there is nothing deep about this explanation. We have instead another just-so story, plausible but not convincing, and, as with all just-so stories, the implication is that things could easily be another way. If the chain of accidents leading to our emergence had taken another route, we might indeed have horny protrusions on our foreheads. If RNA molecules had ascended according to a different route, the threshold of complexity might be different: things simpler than pleuromona might be able to sustain themselves. Or, alternatively, the simplest possible life form might be the mollusk.
In short, the nude RNA or the nude ribozyme polymerase offers no deep account of the observed minimal complexity of all free-living cells. I hold it as a virtue of the origins theory I shall describe in Chapter 3 that it makes it clear why matter must reach a certain level of complexity in order to spring into life. This threshold is not an accident of random variation and selection; I hold that it is inherent to the very nature of life.
The Crystallization of Life
We are not supposed to be here. Life cannot have occurred. Before you get up to leave your chair, your very existence standing as blunt refutation to the argument you are about to hear, simple intellectual politeness invites you to reconsider and linger awhile. The argument I now present has been held seriously by very able scientists. Its failure, I believe, lies in its inability to understand the profound power of self-organization in complex systems. I shall be at pains to show you soon that such self-organization may have made the emergence of life well-nigh inevitable.
We begin on an optimistic note with an argument for the origin of life
on earth from the Nobel laureate George Wald in an article in Scientific American in 1954. Wald wonders how it could be that a collection of molecules came together in just the right way to form a living cell. One has only to contemplate the magnitude of this task to concede that the spontaneous generation of a living organism is impossible. Yet we are here. Wald goes on to argue that, with very many trials, the unthinkably improbable becomes virtually assured. Time is in fact the hero of the plot. The time with which we have to deal is of the order of 2 billion years. (Wald wrote in 1954; we now say 4 billion years.) Given so much time, the impossible becomes possible, the possible probable, and the probable virtually certain. One has only to wait; time itself performs the miracles.
But critics arose, critics of high renown, to argue that even 2 or 4 billion years was not enough time for life to arise by pure 24
    
 happenstance, not by vast orders of magnitude. In his book Origins, Robert Shapiro calculates that in the history of the earth, there could conceivably have been 2.5 X 1051 attempts to create life by chance. That is one hell of a lot of trials. But is it enough? We need to know the probability of success per trial.
Shapiro continues with an effort to calculate the odds of attaining, by chance, something like E. coli. He begins with an argument by two astronomers, Sir Fred Hoyle and N. C. Wickramasinghe. Rather than estimate the chances for obtaining an entire bacterium, these authors try to calculate the chances for obtaining a functioning enzyme. They begin with the set of 20 amino acids that are used to construct enzymes. If the amino acids were selected at random and arranged in random order, what would be the chances of obtaining an actual bacterial enzyme with 200 amino acids? The answer is obtained by
multiplying the probability for each correct amino acid in the sequence, 1 in 20, together 200 times, yielding 1 in 20200, a vastly low probability. But since more than one amino acid sequence might be able to function to catalyze a given reaction, the authors concede a probability of 1 in 1020. But now the coup de grâce: to duplicate a bacterium, it would not suffice to create a single enzyme. Instead, it would be necessary to assemble about 2,000 functioning enzymes. The odds against this would be 1 in 1020 x 2,000, or 1 in 1040,000 These exponential notations are easy to state, but difficult to take to heart. The total number of hydrogen atoms in the universe is something like 1060. So l040,000 is vast beyond vast, unimaginably hyper-astronomical. And 1 in 1040,000 is unthinkably improbable. If the total number of trials for life to get going is only 1051, and the chances are 1 in lO40-000, then life just could not have occurred. We the lucky. We the very, very lucky. We the impossible. Hoyle and Wickramasinghe gave
up on spontaneous generation, since the likelihood of the event was comparable to the chances that a tornado sweeping through a junkyard might assemble a Boeing 747 from the materials therein.
Since you are reading this book, and I am writing it, something must be wrong with the argument. The problem, I believe, is that Hoyle, Wickramasinghe, and many others have failed to appreciate the power of self-organization. It is not necessary that a specific set of 2,000 enzymes be assembled, one by one, to carry out a specific set of reactions. As we shall see in Chapter 3, there are compelling reasons to believe that whenever a collection of chemicals contains enough different kinds of molecules, a metabolism will crystallize from the broth. If this argument is correct, metabolic networks need not be built one component at a time; they can spring full-grown from a primordial soup. Order for free, I call it. If I am right, the motto of life is not We the improbable, but We the expected.
Chapter 3
We the Expected
What raw day first saw life, raw itself, pregnant with the future? Four billion years, thereabouts, from the first circle of metabolic witchery to me and to thee. Raw chance? Raw improbability that ought never have occurred in billions of times the history of this universe? Raw meaninglessness that we are so very unexplained?
Is life really the unthinkable accident that follows from the calculations of Fred Hoyle and N. C. Wickramasinghe? Is time the hero of the plot, as George Wald argued? Yet we now believe there were but 300 million years or so from the cooling of the crust to clear evidence of cellular life, not the 2 billion years that Wald appealed to. Time was not there in sufficient vastness for Wald's story, and surely not for Hoyle and Wickramasinghe's tale. If we the living are wildly improbable, then we are unaccountable mysteries in the span of space and time. But if this view is wrong, if there is some reason to believe that life is probable, then we are not mysteries in the exploding cosmos, we are natural parts of it.
Most of my colleagues believe that life emerged simple and became complex. They picture nude RNA molecules replicating and replicating and eventually stumbling on and assembling all the complicated chemical machinery we find in a living cell. Most of my colleagues also believe that life is utterly dependent on the molecular logic of template replication, the A-T, G-C Watson-Crick pairing that I wrote about in Chapter 2. I hold a renegade view: life is not shackled to the magic of template replication, but based on a deeper logic. I hope to persuade you that life is a natural property of complex chemical systems, that when the number of different kinds of molecules in a chemical soup passes a certain threshold, a self-sustaining network of reactions—an autocatalytic metabolism—will suddenly appear. Life emerged, I suggest, not simple,
    25

   but complex and whole, and has remained complex and whole ever since—not because of a mysterious élan vital, but thanks to the simple, profound transformation of dead molecules into an organization by which each molecule's formation is catalyzed by some other molecule in the organization. The secret of life, the wellspring of reproduction, is not to be found in the beauty of Watson-Crick pairing, but in the achievement of collective catalytic closure. The roots are deeper than the double helix and are based in chemistry itself. So, in another sense, life—complex, whole, emergent—is simple after all, a natural outgrowth of the world in which we live.
The claim that life emerges as a natural phase transition in complex chemical systems is so radical a departure from past theories that I owe you caveats. Do we know that such a view is at least theoretically coherent? Do we know it to be physically and chemically possible? Is there evidence for such a view? Is evidence attainable? Do we know that life began as I shall suggest it did? The most that can be said at this stage is that good, careful theoretical work strongly supports the possibility I shall present. That work appears to be consistent with what we know about complex chemical systems. Scant experimental evidence supports this view as yet, but stunning developments in molecular biology now make it possible to imagine actually creating these self-reproducing molecular systems—synthesized life. I believe that this will be accomplished within a decade or two.
The Networks of Life
As noted in Chapter 2, most researchers are focusing their attention on the capacity of RNA, or RNA-like polymers, to self- reproduce by template replication. The attention is understandable. No one looking at the beautiful double helix of DNA or RNA and regarding the Watson-Crick pairing rules can avoid being struck by the beauty of nature's apparent choice. The fact that Leslie Orgel and his colleagues have not yet succeeded in getting such polymers to replicate without an enzyme does not mean that the efforts will always fail. Orgel has been at it for perhaps 25 years; nature took something like 100 million years. Orgel is very smart, but 100 million years is long enough, measured in three-year National Institutes of Health grants, to try lots of possibilities. Let us try a different tack. Suppose that the laws of chemistry were slightly different, that nitrogen had four rather than five valence electrons, say, allowing four rather than five bonding partners. Ignore the wrench this would throw into quantum mechanics—one can sometimes get away
with being wretched to quantum mechanics when making a philosophical point. If the laws of chemistry were slightly different so that the beautiful double-helix structure of DNA and RNA were no longer possible, would life based on chemistry be impossible? I do not want to think that we were quite so lucky. I hope we can find a basis for life that lies deeper than template self-complementarity.
The secret, I believe, lies in what chemists call catalysis. Many chemical reactions proceed only with great difficulty. Given a long expanse of time, a few molecules of A might combine with molecules of  to make C. But in the presence of a catalyst, another molecule we'll call D, the reaction catches fire and proceeds very much faster. The usual metaphor is the lock and key: A and  fit into slots on D, in just such a way that they are far more likely to combine to form C. As we shall see, this is a vast oversimplification, but for now it will suffice to get the point across. While D is the catalyst that joins A and  to make C, the molecules A, B, and  might themselves act as catalysts for other reactions.
At its heart, a living organism is a system of chemicals that has the capacity to catalyze is own reproduction. Catalysts such as enzymes speed up chemical reactions that might otherwise occur, but only extremely slowly. What I call a collectively autocatalytic system is one in which the molecules speed up the very reactions by which they themselves are formed: A makes ;  makes ;  makes A again. Now imagine a whole network of these self-propelling loops (Figure 3.1). Given a supply of
Figure 3.1 A simple autocatalytic set. Two dimer molecules, AB and  A, are formed from two simple monomers, A and B. Since AB 26
   
 and BA catalyze the very reactions that join As and Bs to make the dimers, the network is autocatalytic: given a supply of "food" molecules (As and Bs), it will sustain itself.
food molecules, the network will be able to constantly re-create itself. Like the metabolic networks that inhabit every living cell, it will be alive. What I aim to show is that if a sufficiently diverse mix of molecules accumulates somewhere, the chances that an autocatalytic system—a self-maintaining and self-reproducing metabolism—will spring forth becomes a near certainty. If so, then the emergence of life may have been much easier than we have supposed.
What I aim to show is simple, but radical. I hold that life, at its root, does not depend on the magic of Watson-Crick base pairing or any other specific template-replicating machinery. Life, at its root, lies in the property of catalytic closure among a collection of molecular species. Alone, each molecular species is dead. Jointly, once catalytic closure among them is achieved, the collective system of molecules is alive.
Each cell in your body, every free-living cell, is collectively autocatalytic. No DNA molecules replicate nude in free-living organisms. DNA replicates only as part of a complex, collectively autocatalytic network of reactions and enzymes in cells. No RNA molecules replicate themselves. The cell is a whole, mysterious in its origins perhaps, but not mystical. Except for "food molecules," every molecular species of which a cell is constructed is created by catalysis of reactions, and the catalysis is itself carried out by catalysts created by the cell. To understand the origin of life, I claim, we must understand the conditions that enabled the first emergence of such autocatalytic molecular systems.
Catalysis alone, however, is not sufficient for life. All living systems "eat": they take in matter and energy in order to reproduce themselves. This means that they are what is referred to in Chapter 1 as open thermodynamic systems.
In contrast, closed thermodynamic systems take in no matter or energy from their environments. A great deal is understood about the behavior of closed thermodynamic systems. The theorists of thermodynamics and statistical mechanics have studied such systems for over 100 years. In contrast, remarkably little is understood about the possible behaviors of open thermodynamic systems. Not so surprising, this ignorance. The vast flowering of all life-forms over the past 3.45 billion years is merely a hint of the possible behaviors of open thermodynamic systems. So too is cosmogenesis itself, for the evolving universe since the Big Bang has yielded the formation of galactic and supragalactic structures on enormous scales. Those stellar structures and the nuclear processes within stars, which have generated the atoms and molecules from which life itself arose, are open systems, driven by nonequilibrium processes. We have only begun to understand the awesome creative powers of nonequilibrium processes in the unfolding universe. We are
all—complex atoms, Jupiter, spiral galaxies, warthog, and frog—the logical progeny of that creative power.
Since I hope to persuade you that life is the natural accomplishment of catalysts in sufficiently complex nonequilibrium chemical systems, I had best take a moment to sketch what catalysts accomplish and how equilibrium and nonequilibrium chemical systems behave. Chemical reactions occur spontaneously, some rapidly, some slowly. Typically, chemical reactions are more or less reversible: A transforms to B, but  transforms to A. Since such reactions are reversible, it is easy to think about what would occur in a beaker that began with an initial concentration of A molecules and no  molecules and that was closed to the addition of matter or energy. The A molecules would begin to convert to  molecules, but as that occurred, the new  molecules would begin to convert back to A molecules. Starting with only A molecules, the  concentration would build up to the point at which the rate of conversion of A to  was exactly equal to the rate of conversion of  to A. This balance is called chemical equilibrium. At chemical equilibrium, the net concentrations of A and  do not change over time, but any given A molecule may convert to  and back again thousands of times per minute. Of course, the equilibrium is statistical. Minor fluctuations in A and  concentrations occur all the time.
Chemical equilibrium is not limited to a pair of molecules, A and B, but will occur in any closed thermodynamic system. If the system has hundreds of different types of molecules, it will ultimately settle down to an equilibrium in which the forward and reverse reactions between any pair of molecules balance out.
Catalysts, of which protein enzymes and ribozymes are examples, can speed up both the forward and the reverse reaction by the same amount. The equilibrium between A and  is not altered; enzymes simply hasten the rate at which this state of balance is reached. Suppose, at equilibrium, the ratio of A and  concentrations is 1, so the concentrations of the two are equal. If the chemical system starts out displaced from equilibrium—say, with a high concentration of  and almost no A—then the enzyme will vastly shorten the time it takes to reach the equilibrium ratio where the two concentrations are
    27

 equal. In effect, then, the enzyme increases the rate of production of A.
How does catalysis happen? There is an intermediate state between A and B, called the transition state, in which one or more bonds among the atoms of the molecule are severely strained and distorted. The transition-state molecule is therefore rather unhappy. The measure of this unhappiness is given by the energy of the molecule. Low energy corresponds to unstrained molecules. High energy corresponds to strained
molecules. Think of a spring. At its rest length, it is happy. If stretched beyond its rest length, it has stored energy—it is unhappy—and can release that energy by snapping back to its rest length, whereupon it has low energy again.
Not surprisingly, the transition state passing from A to  is exactly the same as the transition state passing from  back to A. Enzymes are thought to work by binding to the transition state and stabilizing it. This makes it easier for both A and  molecules to jump to the transition state, increasing the rate of conversion of A to B, and of  to A. Thus an enzyme increases the rate at which the equilibrium ratio of A and  concentrations is approached.
We should be thankful that our cells are not at chemical equilibrium; for a living system, equilibrium corresponds to death. Living systems are, instead, open thermodynamic systems persistently displaced from chemical equilibrium. We eat and excrete, as did our remote ancestors. Energy and matter flow through us, building up the complex molecules that are the tokens in the game of life.
Open nonequilibrium systems obey very different rules from those of closed systems. Consider a simple case: we have a beaker into which we add A molecules continuously from some outside source at a constant rate, and we take any  molecules out of the beaker at a rate proportional to the concentration of B. A will convert to  and  will convert to A as before, but the two molecules can never reach the equilibrium balance they attained before because of the constant addition of A and the removal of B. Common sense says that the system will settle down to a steady state at which the ratio of A molecules to  molecules is higher than it was when the system was closed. In short, the ratio of A to  will be tipped from the thermodynamic equilibrium ratio. In general, this commonsense view is correct. In simple cases, such systems, open to the flux of matter and energy, settle down to a steady state different from that found in closed thermodynamic systems.
Now consider a vastly more complex open system, the living cell. The cells of your body coordinate the behaviors of about 100,000 different kinds of molecules as matter and energy cross their boundaries. Even bacteria coordinate the activities of thousands of different kinds of molecules. To think that understanding the behavior of very simple open thermodynamic chemical systems takes us far toward understanding the cell is hubris. No one understands how the complex cellular networks of chemical reactions and their catalysts behave, or what laws might govern their behavior. Indeed, this is a mystery we will begin to discuss in the next chapter. Yet simple open thermodynamic systems are at least a start and are already fascinating on their own. Even simple nonequilibrium chemical systems can form remarkably complex patterns of
chemical concentrations varying in time and space in striking ways. As noted in Chapter 1, Ilya Prigogine called these systems dissipative because they persistently dissipate matter and energy in order to maintain their structures.
Unlike the simple steady-state system in the thermodynamically open beaker, the concentrations of the chemical species in a more complex dissipative system may not fall to a steady state, unchanging in time. Instead, the concentrations can start to oscillate up and down in repeated cycles, called limit cycles, which are sustained for long periods of time. Such systems can also generate remarkable spatial patterns. For example, the famous Belosov-Zhabbtinski reaction, made of some simple organic molecules, sets up two kinds of spatial patterns. In the first pattern, spreading concentric circular waves of blue propagate outward over an orange background from a central oscillating source. The blue and orange colors arise because of indicator molecules that track how acidic or basic the reaction mixture is at any point in space. In the second pattern, spiral pinwheels of blue on orange cartwheel about a center (Figure 3.2). Such patterns have been studied by a number of researchers. A fine book by my friend Arthur Winfree, When Time Breaks Down: The Three-Dimensional Dynamics of Electrochemical Waves and Cardiac Arrhythmias, summarizes much of the work. Among the most immediate human implications is this: the heart is an open system, and it can beat according to patterns analogous to the Belosov-Zhabotinski
    28

  a
b
Figure 3.2 Self-organization at work. The famous Belosov-Zhabotinski reaction showing the spontaneous emergence of order in a simple chemical system, (a) Concentric circular waves propagate outward, (b) Radially expanding pinwheels cartwheel about a center.
reaction. Sudden death caused by cardiac arrhythmias may correspond to a switch from the analogue of the concentric- circles pattern (a steady beating) to the spiral-pinwheels pattern in your myocardium. The blue propagating wave can be thought of as corresponding to the chemical conditions in muscle cells that lead them to contract. Thus the concentric spreading pattern of the evenly spaced blue circles corresponds to ordered contraction waves. But in the spiral pattern, the blue pinwheels are very close together near the center of the spiral and are spaced farther apart the farther out on the spiral they go. This pattern corresponds to chaotic twitching of the heart muscle in the vicinity of the spiral center. Winfree has shown that simple perturbations, such as shaking the petri plate that holds the chemical reactants of the Belosov- Zhabotinski reaction, can switch the system from the concentric to the spiral pattern. Thus Winfree has suggested that simple perturbations can switch a normal heart to the spiral chaotic pattern and lead to sudden death.
The relatively simple behaviors of nonequilibrium chemical systems are well studied and may have a variety of biological implications. For example, such systems can form a standing pattern of stripes of high chemical concentrations spaced between stripes of low chemical concentrations. Many of us think that the natural patterns such systems form have a great deal to tell us about the spatial patterning that occurs in the development of plants and animals. The blue and orange stripes in the Belosov-Zhabotinski reaction may foretell the stripes of the zebra, the banding patterns on shells, and other aspects of morphology in simple and complex organisms.
However intriguing such chemical patterns may be, they are not yet living systems. The cell is not only an open chemical system, but a collectively autocatalytic system. Not only do chemical patterns arise in cells, but cells sustain themselves as reproducing entities that are capable of Darwinian evolution. By what laws, what deep principles, might autocatalytic systems have emerged on the primal earth? We seek, in short, our creation myth.
A Chemical Creation Myth 29
   
 Scientists often gain insight into a more complex problem by thinking through a simpler toy problem. The toy problem I want to tell you about concerns "random graphs." A random graph is a set of dots, or nodes, connected at random by a set of lines, or edges. Figure 3.3 shows an example. To make the toy problem concrete, we can call the dots
Figure 3.3 Crystallization of connected webs. Twenty "buttons" (nodes) are connected at random by an increasing number of "threads" (edges). For large numbers of buttons, as the ratio of threads to buttons increases past a threshold of"0,5, most points become connected in one giant component. As the ratio passes 1.0, closed pathways of all lengths begin to emerge.
"buttons" and the lines "threads." Imagine 10,000 buttons scattered on a hardwood floor. Randomly choose two buttons and connect them with a thread. Now put this pair down and randomly choose two more buttons, pick them up, and connect them with a thread. As you continue to do this, at first you will almost certainly pick up buttons that you have not picked up before. After a while, however, you are more likely to pick at random a pair of buttons and find that you have already chosen one of the pair. So when you tie a thread between the two newly chosen buttons, you will find three buttons tied together. In short, as you continue to choose random pairs of buttons to connect with a thread, after a while the buttons start becoming interconnected into larger clusters. This is shown in Figure 3.3a, which is limited to 20 rather than 10,000 buttons. Every now and then, lift up a button and see how many other buttons you pick up. The connected cluster is called a component in our random graph. As Figure 3.3a shows, some buttons may not be connected to any other buttons. Other buttons might be connected in pairs or triples or larger numbers.
The important features of random graphs show very regular statistical behavior as one tunes the ratio of threads to buttons. In particular, a phase transition occurs when the ratio of threads to buttons passes 0.5. At that point, a "giant cluster" suddenly forms. Figure 3.3 shows this process, using only 20 buttons. When there are very few threads compared with the number of buttons, most buttons will be unconnected (Figure 33a), but as the ratio of threads to buttons increases, small connected clusters begin to form. As the ratio of threads to buttons continues to increase, the size of these clusters of buttons tends to grow. Obviously, as clusters get larger, they begin to become cross-connected. Now the magic! As the ratio of threads to buttons passes the 0.5 mark, all of a sudden most of the clusters have become cross-connected into one giant structure. In the small system with 20 buttons in Figure 3.3, you can see this giant cluster forming when the ratio of threads to buttons is half, 10 threads to 20 buttons. If we used 10,000 buttons, the giant component would arise when there were about 5,000 threads. When the giant component forms, most of the nodes are directly or indirectly connected. If you pick up one button, the chances are high that you will pull up something like 8,000 of the 10,000 buttons. As the ratio of threads to buttons continues to increase past the halfway mark, more and more of the remaining isolated buttons and small clusters become cross-connected into the giant component. So the giant component grows larger, but its rate of growth slows as the number of remaining isolated buttons and isolated small components decreases.
           30

 The rather sudden change in the size of the largest connected cluster of buttons, as the ratio of threads to buttons passes 0.5, is a toy version of the phase transition that I believe led to the origin of life. In Figure 3.4, I show qualitatively the size of the largest cluster among 400 nodes as the ratio of edges to nodes increases. Note that the curve is S-shaped, or sigmoidal. The size of the largest cluster of nodes increases slowly at first, then rapidly, then slows again as the ratio of edge to nodes increases. The rapid increase is the signature of something like a phase transition (Figure 3.4). In the example in Figure 3.4 using 400 buttons, the sigmoidal curve rises steeply when the ratio of edges to nodes passes 0.5. The steepness of the curve at the critical 0.5 ratio depends on the number of nodes in the system. When the number of nodes is small, the steepest part of the curve is "shallow," but as the number of nodes in the toy system increases—from, say, 400 to 100 million—the steep part of the sigmoidal curve becomes more vertical. Were there an infinité number of buttons, then as the ratio of threads to buttons passed 0.5 the size of the largest component would jump discontinuously from tiny to enormous. This is a phase transition, rather like separate water molecules freezing into a block of ice.
Figure 3.4 A phase transition. As the ratio of threads (edges) to buttons (nodes) in a random graph passes 0.5, the size of the connected cluster slowly increases until it reaches a "phase transition" and a giant component crystallizes. (For this experiment, the number of threads ranges from 0 to 600, while the number of buttons is fixed at 400.)
The intuition I want you to take away from this toy problem is simple: as the ratio of threads to buttons increases, suddenly so many buttons are connected that a vast web of buttons forms in the system. This giant component is not mysterious; its emergence is the natural, expected property of a random graph. The analogue in the origin-of-life theory will be that when a large enough number of reactions are catalyzed in a chemical reaction system, a vast web of catalyzed reactions will suddenly crystallize. Such a web, it turns out, is almost certainly au-tocatalytic—almost certainly self-sustaining, alive.
Reaction Networks
It is convenient to draw a metabolic reaction graph with circles representing chemicals and square representing reactions. To be concrete, we will consider four simple kinds of reactions. In the simplest, one substrate, A, converts to one product, B. Since reactions are reversible,  also converts back to A. This is a one-substrate, one-product reaction. Draw a black line leaving A and entering a small square lying between A and B, and draw a line leaving the square and ending on  (Figure 3.5). This line and the square represent the reaction between A and B. Now consider two molecules, say A and B, that are combined, or "ligated," to form a larger molecule, C. In the reverse reaction,  is "cleaved" to form A and B. We can represent these reactions with two lines leaving A and  and entering a square representing this reaction, plus a line leaving the square and entering C. Finally, we should consider reactions with two substrates and two products. Typically, this kind of reaction occurs by breaking off a small cluster of atoms from one substrate and bonding the cluster to one or more atoms on the second substrate. We can represent two-substrate, two-product reactions with pairs of lines leaving the two substrates and entering a square representing that reaction, and two more lines leaving the square and connecting to the two products. Now consider all the kinds of molecules and reactions possible in a chemical reaction system. The collection of all such lines and squares between all the chemical circles constitutes the reaction graph (Figure 3.5).
Since we want to understand the emergence of collectively autocat-alytic molecular systems, the next step is to distinguish between spontaneous reactions, which are assumed to occur very slowly, and catalyzed reactions, which are assumed to occur rapidly. We want to find the conditions under which the same molecules will be catalysts for and products of the reactions creating the autocatalytic set. This depends on the
     31

  Figure 3.5 From buttons and threads to chemicals. In this hypothetical network of chemical reactions, called a reaction graph, smaller molecules (A and B) are combined to form larger molecules (AA, AB, etc.), which are combined to form still larger molecules (BAB, BBA, BABB, etc.). Simultaneously, these longer molecules are broken down into simple substrates again. For each reaction, a line leads from the two substrates to a square denoting the reaction; an arrow leads from the reaction square to the product. (Since reactions are reversible, the use of arrows is meant to distinguish substrates from products in only one direction of the chemical flow.) Since the products of some reactions are substrates of further reactions, the result is a web of interlinked reactions.
possibility that each molecule in the system can play a double role: it can serve as an ingredient or a product of a reaction, but it can also serve as a catalyst for another reaction. This dual role, as ingredient or catalyst, is perfectly possible, even farrriliar. Proteins and RNA molecules are known to play such a dual role. An enzyme called trypsin cleaves proteins you eat into smaller fragments. In fact, trypsin will cleave itself into fragments as well. And, as noted in Chapter 2, ri-bozymes are RNA molecules that can act as enzymes on RNA molecules. It is perfectly familiar that all kinds of organic molecules can be
substrates and products of reactions, but simultaneously act catalytically to hasten other reactions. No mystery stands in the way of a dual role for chemicals.
To proceed further, we need to know which molecules catalyze which reactions. If we knew this, we could tell whether any set of molecules might be collectively autocatalytic. Unfortunately, this knowledge is not, in general, yet available, but we can sensibly proceed by making plausible assumptions. I will consider two such simple theories, each of which allows us, in the model worlds we will consider, to assign, somewhat arbitrarily, catalysts to reactions. You should be skeptical about this maneuver. Surely, it might be thought, one must actually know which molecules catalyze which reactions to be certain that a set of molecules harbors an autocatalytic set. Such skepticism is well placed and allows me to introduce a mode of reasoning on which I am depending. One might easily object that if in the real world of chemical reactions the laws of chemistry dictated a somewhat different distribution of which molecules catalyzed which reactions, then the conclusions would not hold. My response is this: if we can show that for many alternative "hypothetical" chemistries, in which different molecules catalyze different reactions, autocatalytic sets emerge, then the particular details of the chemistry may not matter. We will be showing that the spontaneous emergence of self-sustaining webs is so natural and robust that it is even deeper than the specific chemistry that happens to exist on earth; it is rooted in mathematics itself.
Picture, as noted earlier, a reaction between a pair of molecules, A and B, as black lines or edges connecting A and  to the reaction square between them. Now picture some other molecule, C, that is able to catalyze the reaction between A and B. Represent this by drawing a blue arrow with its tail in  and its head on the reaction square between A and  (Figure 3.6). Represent the fact that the reaction between A and  is catalyzed by changing the black line between A and  to a red line. Consider each molecule in the system, and ask which reaction or reactions, if any, it can catalyze. For any such catalyst,
  32

 draw a blue arrow to the corresponding reaction square, and color the corresponding reaction edges red. When you have finished this task, the red edges and the chemical nodes they connect represent all the catalyzed reactions, and collectively make up the catalyzed reaction subgraph of the whole reaction graph. The blue arrows and the chemical nodes from which they leave represent the molecules that carry out the catalysis (Figure 3.6).
Now consider what is required for the system to contain an autocatalytic subset: first, a set of molecules must be connected by red catalyzed reactions; second, the molecules in this set must each have its
Figure 3.6 Molecules catalyzing reactions. In Figure 3.5, all the reactions were assumed to be spontaneous. What happens when we add catalysts to speed some of the reactions? Here the reaction squares indicated by dashed-line arrows are catalyzed, and the heavy, darker lines connect substrates and products whose reactions are catalyzed. The result is a pattern of heavy lines indicating a catalyzed subgraph of the reaction graph.
formation catalyzed by a blue arrow from some molecule in the same set or be added from outside. Call the latter molecules food molecules. If these conditions are met, we have a network of molecules that can catalyze its own formation, creating all the catalysts it needs.
The Central Idea
How likely is it that such a self-sustaining web of reactions would arise naturally? Is the emergence of collective autocatalysis easy or virtually impossible? Do we have to pick our chemicals carefully, or would just about any mixture do? The answer is heartening. The emergence of au-tocatalytic sets is almost inevitable.
Here, in a nutshell we will unpack later, is what happens: as the diversity of molecules in our system increases, the ratio of reactions to chemicals, or edges to nodes, becomes ever higher. In other words, the reaction graph has ever more lines connecting the chemical dots. The molecules in the system are themselves candidates to be able to catalyze the reactions by which the molecules themselves are formed. As the ratio of reactions to chemicals increases, the number of reactions that are catalyzed by the molecules in the system increases. When the number of catalyzed reactions is about equal to the number of chemical dots, a giant catalyzed reaction web forms, and a collectively autocat-alytic system snaps into existence. A living metabolism crystallizes. Life emerges as a phase transition.
Now we will unpack our nutshell.
The first step is to show that as the diversity and complexity of the molecules in our system increase, the ratio of reactions
     33

 to chemical dots in the reaction graph increases as well. It is easy to see why this is true. Consider a polymer consisting of four "monomers," which we can think of as atoms ABBB. Clearly, the polymer can be formed by gluing A to BBB, by gluing AB to BB, or by gluing ABB to B. So it can be formed in three ways, by three different reactions. If we increase the length of the polymer by one atom, the number of reactions per molecule will rise. ABBBA can be formed from A and BBBA, AB and BBA, ABB and BA, or ABBB and A. Since a polymer of length L has L — 1 internal bonds in general, a polymer of length L can be formed from smaller polymers in L — 1 ways. But these numbers account for only what chemists call ligation reactions, building up molecules from smaller pieces. Molecules can also be formed through cleavage. ABBB can be formed by lopping the A from the right-hand side of ABBBA. So it is rather obvious that there are more reactions by which molecules can be formed than there are molecules themselves. This means that in the reaction graph there are more lines than dots.
What happens to the ratio of reactions to molecules in the reaction graph as the diversity and complexity of those molecules increase? After some simple algebra, it is easy to show for simple linear polymers that as the length of the molecules increases, the number of kinds of molecules increases exponentially, but the number of reactions by which they convert from one to another rises even faster. This increasing ratio means that as more complex and diverse sets of molecules are considered, the reaction graph among them becomes ever denser with paths by which they can change from one into another. The ratio of reaction "lines" to dots becomes denser, a black forest of possibilities. The chemical system becomes ever more fecund with reactions by which molecules transform into other molecules.
At this point we have a flask of slow, spontaneous reactions. For the system to catch fire and generate self-sustaining autocatalytic networks, some of the molecules must act as catalysts, speeding up the reactions. The system is fecund, but not yet pregnant with life, and will not become so until we have a way to determine which molecules catalyze which reactions. Thus it is time to build some simple models. The simplest, which will do very well for a variety of purposes, is to assume that each polymer has a fixed chance, say one in a million, of being able to function as an enzyme to catalyze any given reaction. In using this simple model, we will "decide" which reactions, if any, each polymer can catalyze by flipping a biased coin that comes up heads once in a million times. Using this rule, any polymer will be randomly assigned, once and for all, the reactions it can catalyze. Using this "random catalyst" rule, we can "color" the catalyzed reactions red, draw our blue arrows from the catalysts to the reactions each catalyzes, and then ask whether our model chemical system contains a collectively autocatalytic set: a network of molecules connected by red lines and also containing the very molecules that catalyze, via the blue arrows, the reactions by which the molecules themselves are formed.
A somewhat more chemically plausible model supposes that our polymers are RNA sequences and introduces template matching. In this simplified version, Bs fit with As in a kind of Watson-Crick pairing. Thus the hexamer BBBBBB might be able to act like a ribozyme and bind two substrates, BABAAA and AAABBABA, by their two corresponding AAA trimer sites, and catalyze the ligation of the two substrates to form BABAAAAAABBABA. To make things even more chemically realistic, we might also demand that even if a candidate ribozyme has a site that matches the left and right ends of its substrates, it still has only one chance in a million to have other chemical properties that allow it to catalyze the reaction. This captures the idea that other chemical features beyond template matching may be required to achieve ribozyme catalysis. Let us call this the match catalyst rule.
Here is the crucial result: no matter which of these "catalyst" rules we use, when the set of model molecules reaches a critical diversity, a giant "red" component of catalyzed reactions crystallizes, and so collectively autocatalytic sets emerge. Now it is easy to see why this emergence is virtually inevitable. Suppose we use the random catalyst rule and assume that any polymer has a one-in-a-million chance to act as an enzyme for any given reaction. As the diversity of molecules in the model system increases, the ratio of reactions to molecules increases. When the diversity of molecules is high enough, the ratio of reactions to polymers reaches a million to one. At that diversity, on average each polymer will catalyze one reaction. A million to one multiplied by one
in a million equals one. When the ratio of catalyzed reactions to chemicals is 1.0, then with extremely high probability a "red" giant component, a web of catalyzed reactions, will form—a collectively autocat-alytic set of molecules.
In this view of the origin of life, a critical diversity of molecules must be reached for the system to catch fire, for catalytic closure to be attained. A simple system with 10 polymers in it and a chance of catalysis of one in a million is just a set of dead molecules. Almost certainly, none of the 10 molecules catalyzes any of the possible reactions among the 10 molecules. Nothing happens in the inert soup save the very slow spontaneous chemical reactions. Increase the diversity and atomic complexity of the molecules, and more and more of the reactions among them become catalyzed by members of the system itself. As a threshold diversity is crossed, a giant web of catalyzed reactions crystallizes in a phase transition. The
    34

 catalyzed reaction subgraph goes from having many disconnected tiny components to having a giant component and some smaller, isolated components. Your intuitions may now be tuned enough to guess that the giant component will contain a collectively au-tocatalytic subset able to form itself by catalyzed reactions from a supply of food molecules.
I have now related the central ideas about how I think life may have formed. These ideas are really very simple, if unfamiliar. Life crystallizes at a critical molecular diversity because catalytic closure itself crystallizes. These ideas, I hope, will become experimentally established parts of our new chemical creation story, our new view of our ancient roots, our new sense of the emergence of life as an expected property of the physical world.
In the computer-simulation movies we have made of this process, we can see this crystallization happening through an increase in either the diversity of molecules or the probability that any molecule catalyzes any reaction. We call these parameters M and P. As either M or P increases, at first nothing much happens in the dead soup; then suddenly it springs to life. The experiment has not been done with real chemicals yet, although I'll return to that later. But on the computer, a living system swarms into existence. Figure 3.7 shows what one of these model self-reproducing metabolisms actually looks like. As you can see, this model system is based on the continuous supply of several simple food molecules, the monomers A and B, and the four possible dimers: AA, AB, BA, and BB. From this, the system crystallizes a collectively auto-catalytic, self-sustaining model metabolism with some 21 kinds of molecules. More complex autocatalytic sets have hundreds or thousands of molecular components.
Figure 3.7 An autocatalytic set. A typical example of a small autocatalytic set in which food molecules (a, h, aa, bb) are built up into a self-sustaining network of molecules. The reactions are represented by points connecting larger polymers to their breakdown products. Dotted lines indicate catalysis and point from the catalyst to the reaction being catalyzed.
The same basic results are found if we use the template-matching model of catalysis. The ratio of possible reactions to polymers is so vast that eventually a giant catalyzed component and autocatalytic sets emerge. Given almost any way in which nature might determine which chemicals catalyze which reactions, a critical molecular diversity is reached at which the number of red catalyzed reactions passes a phase transition and a vast web of chemicals crystallizes in the system. This vast web is, it turns out, almost always collectively autocatalytic.
Such a system is, at minimum, self-sustaining, but such a system is very nearly self-reproducing. Suppose our collectively autocatalytic reaction system is contained within some kind of compartment. Compartmentalization must have been essential to prevent dilution of the reacting molecules. The autocatalytic system might constitute one of Alexander Oparin's coascervates, or it might create and be contained in a bilipid membrane vesicle. As the molecular constituents of the system re-create themselves, the number of copies of each kind of molecule can increase until the total has doubled. The system can then break into two coascervates, two bilipid membrane vesicles, or other compartmentalized forms. In fact, such
      35

 breaking in two happens spontaneously as such systems increase in volume. Thus our autocatalytic protocell has now self- reproduced. A self-reproducing chemical system, alive by these criteria, springs into existence.
Energizing the Reactions
Now one might object that what is true for As and Bs may not be true for atoms and molecules. As Einstein said, a theory should be as simple as possible, but not too simple. One thing lacking in our model so far has been energy. As we have seen, living systems are open, nonequilibrium thermodynamic systems, sustained by a flux of matter and energy through them. As with the vastly simpler Belosov-Zhabotinski reaction, living systems maintain structures by dissipating matter and energy—in short, by eating and excreting.
The problem is this: it takes energy to create large polymers, for thermodynamics favors their breakdown into smaller constituents. A chemically realistic autocatalytic set has to obtain energy to create and sustain large molecules that may be its catalysts.
To be concrete, consider a protein with 100 amino acids linked together, or even a smaller sequence of amino acids, called a peptide. The linking of any two amino acids by a peptide bond requires energy. An easy way to see this is that the bond confines the motion of the two amino acids relative to each other. It would require some tugging to pull the amino acids apart. The tugging required is a measure of the energy of the bond. I noted earlier that almost all reactions are spontaneously reversible. This is true of a peptide bond. During its formation, a water molecule is pulled out of the reacting pair of amino acids. Thus water itself is a product of the reaction. Conversely, when a peptide bond is cleaved, a water molecule is used up. If peptides are dissolved in water, the water molecules will tend to break peptide bonds.
In a normal aqueous environment, the equilibrium ratio of cleaved amino acids to amino acid pairs (dipeptides) is about 10 to 1. But the same calculation holds for a dipeptide plus a single amino acid coming together to form a tripeptide. In an aqueous environment, the ratio of the dipeptide and amino acid to the tripeptide will be about 10 to 1 at chemical equilibrium. Note the consequence: at equilibrium, the ratio of two amino acids to the dipeptide they form is 10 to 1, and the ratio of the dipeptide plus a single amino acid to the tripeptide is also 10 to 1. Thus the ratio of single amino acids to tripeptides is not 10 to 1, but roughly 100 to 1. Similarly, at equilibrium, the ratio of amino acids to tetrapeptides is about 1,000 to 1. As the bigger polymer increases in length, its equilibrium concentration relative to the amino acids falls by a factor of about 10-fold for each increase of one amino acid in length.
The implication of the previous simple calculation is this: in an equilibrium mixture of single amino acids and various peptides up to length, say 25, the average ratio of the amino acid concentrations to that of any specific peptide of 25 amino
acids would be about 1 to 10-25. To be concrete, if amino acids were dissolved to the highest concentration in water that can be attained, then at equilibrium the number of copies of any specific sequence of amino acids 25 residues long would be less than one molecule in a liter of water! By contrast, the number of copies of any of the single amino acids might be on
the order of 1020 to 1023. Autocatalytic sets may use large polymers. How can high concentrations of such molecules be achieved in the face of this thermodynamic difficulty?
There are at least three fundamental ways that this vast obstacle might have been overcome. Each is remarkably simple. First, reactions can be confined to a surface rather than occurring in a volume. The reason this helps form larger polymers is simple. The rate at which a chemical reaction occurs depends on how rapidly the reaction partners collide with one another. If an enzyme is involved, the enzyme must be encountered as well. If the reaction is occurring in a volume, such as a beaker, then each molecule must diffuse in three dimensions and bump into its reaction partners. It is rather easy for molecules wandering in three dimensions to keep missing one another (recall the cartoon I described in Chapter 2). By contrast, if the molecules are confined to a very thin surface layer, such as clay or a bilipid membrane, then the search occurs in only two dimensions. It is a lot harder for the molecules to miss one another. To tune your intuition, imagine the molecules diffusing in a one-dimensional tube with a tiny diameter. Then they are bound to run into one another. In short, confining reactions to occur on surfaces strongly increases the chances of substrates hitting one another, hence enhancing the rate of formation of longer polymers.
A second simple mechanism to enhance the formation of longer polymers is to dehydrate the system. Dehydration removes water molecules, hence slowing down the cleavage of peptide bonds. In computer simulations with my colleagues Doyne Farmer, Norman Packard, and, later, Richard Bagley, we found strong evidence that even simple dehydration ought to suffice to allow real autocatalytic systems of polymers to reproduce. Our model fits the laws of chemistry and physics
    36

 without straining.
Dehydration is not a cheat; it actually works. A famous reaction, called the plastein reaction, was well studied beginning almost 60 years ago. The enzyme trypsin in the stomach helps digest the proteins we eat. If trypsin is mixed with large proteins in an aqueous medium, it cleaves the proteins into smaller peptides. But if the reaction system is dehydrated, lowering the concentration of water relative to the peptides, the equilibrium shifts in favor of the synthesis of larger polymers from the small peptide fragments. Trypsin obliges by catalyzing these ligation reactions, yielding larger polymers. If these larger polymers are removed and the system is again dehydrated, trypsin obliges by synthesizing still more large polymers.
Reactions on surfaces and dehydration can be used to favor the formation of large polymers. But contemporary cells also use a more flexible and sophisticated mechanism. As cells form bonds, they obtain the needed energy by simultaneously breaking down the high-energy bonds in ubiquitous helper molecules. Adenosine triphosphate (ATP) is the most common of these. Reactions that require energy are called ender-gonic; those that release energy are called exergonic. Cells drive ender-gonic reactions by linking them to exergonic reactions.
A number of plausible candidates have been suggested for high-energy bonds that may have powered early self- reproducing metabolisms. For example, pyrophosphate, two phosphates linked together, is abundant and releases substantial energy upon cleavage. Pyrophosphate may have been a useful source of free energy to drive synthesis in early living systems. Farmer and Bagley have used computer simulations to show that model systems powered by these bonds meet plausible thermodynamic criteria and can reproduce.
What is required to link exergonic and endergonic reactions? Does some new mystery confront us beyond the achievement of catalytic closure? I think not. A problem is here, but hardly a mystery. All that is required, after all, is that the autocatalytic set include catalysts that link exergonic and endergonic reactions, so that one powers the other. The endergonic synthesis of large molecules must be coupled with the degradation of high-energy bonds supplied by food molecules or, ultimately, sunlight. But this does not seem an overwhelming obstacle.
Catalysis of such coupled reactions is not fundamentally different from other reactions: an enzyme able to bind the transition state is needed. All that is required is a sufficient diversity of molecules.
An Unrepentant Holism
This theory of life's origins is rooted in an unrepentant holism, born not of mysticism, but of mathematical necessity. A critical diversity of molecular species is necessary for life to crystallize. Simpler systems simply do not achieve catalytic closure. Life emerged whole, not piecemeal, and has remained so. Thus unlike the dominant nude RNA view of the origin of life, with its evolutionary just-so stories, we have a hope of explaining why living creatures seem to have a minimal complexity, why nothing simpler than the pleuromona can be alive.
If this view is right, we should be able to prove it. We should be able to create life anew in the fabled test tube, as though it were held by some scientist driven by Faustian dreams. Can we hope to make a new life-form? Can we brazen the face of God? Yes, I think so. And God, in his grace and simplicity, should welcome our struggles to find his laws. The ways of science are genuinely mysterious. As we shall see in Chapter 7, the hope to create collectively autocatalytic sets of molecules is linked to what may become the second era of biotechnology, promising new drugs, vaccines, and medical miracles. And the concept of catalytic closure in collectively autocatalytic sets of molecules will begin to appear as a deep feature of the laws of complexity, reemerging in our understanding of ecosystems, economic systems, and cultural systems.
Immanuel Kant, writing more than two centuries ago, saw organisms as wholes. The whole existed by means of the parts; the parts existed both because of and in order to sustain the whole. This holism has been stripped of a natural role in biology, replaced with the image of the genome as the central directing agency that commands the molecular dance. Yet an autocatalytic set of molecules is perhaps the simplest image one can have of Kant's holism. Catalytic closure ensures that the whole exists by means of the parts, and they are present both because of and in order to sustain the whole. Autocatalytic sets exhibit the emergent property of holism. If life began with collectively autocatalytic sets, they deserve awed respect, for the flowering of the biosphere rests on the creative power they unleashed on the globe—awed respect and wonder, but not mysticism.
Most important of all, if this is true, life is vastly more probable than we have supposed. Not only are we at home in the universe, but we are far more likely to share it with as yet unknown companions.
  37
